<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>odl</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<p><img src="images/redhat.png" style="width: 200px;" border=0/></p>

<p><font color="red">
<strong>Lab Update - 4th June 2018</strong></p>

<p><strong>This lab has now been updated to run on the Red Hat Product Demo System (RHPDS) and so Summit instructions have been removed in favour of specific instructions for RHPDS. You can skip to the first lab section if you&#39;re following this post-Summit. If you have any questions or any problems accessing this content, please let us know.</strong>
</font></p>

<h2>Red Hat Summit, San Francisco 2018</h2>

<p><strong>Title</strong>: Hands on with OpenStack and OpenDaylight SDN (<strong>L1059</strong>)<br>
<strong>Date</strong>: 8th May 2018<br></p>

<p><strong>Authors/Lab Owners</strong>:
<ul class="tab">
<li>Rhys Oxenham &lt;<a href="mailto:roxenham@redhat.com">roxenham@redhat.com</a>&gt;</li>
<li>Nir Yechiel &lt;<a href="mailto:nyechiel@redhat.com">nyechiel@redhat.com</a>&gt;</li>
<li>Andre Fredette &lt;<a href="mailto:afredette@redhat.com">afredette@redhat.com</a>&gt;</li>
<li>Tim Rozet &lt;<a href="mailto:trozet@redhat.com">trozet@redhat.com</a>&gt;</li>
</ul></p>

<h1 id="toc_0"><strong>Lab Contents</strong></h1>

<ol>
<li><strong>What is OpenDaylight and what can it do?</strong></li>
<li><strong>How does OpenDaylight integrate with OpenStack?</strong></li>
<li><strong>Getting Started with the Labs</strong></li>
<li><strong>Prebuilt Lab Investigation</strong></li>
<li><strong>Testing, investigating, and using OpenDaylight</strong></li>
<li><strong>Exploration of TripleO requirements for OpenDaylight</strong></li>
</ol>

<!--BREAK-->

<h1 id="toc_1"><strong>Lab Overview</strong></h1>

<p>First of all, it&#39;s my pleasure to welcome you to the Red Hat Summit 2018, here at the San Francisco Moscone Centre! The past few years have been an exciting time for Red Hat, and both the OpenStack and OpenDaylight communities; we&#39;ve seen unprecedented interest and development in these new revolutionary technologies and we&#39;re proud to be at the heart of it all. Red Hat is firmly committed to the future of OpenStack and OpenDaylight; our goal is to continue to enhance the technology, make it more readily consumable and to enable our customers to be successful when using it.</p>

<p>This hands-on lab aims to get you, the attendees, a bit closer to both Red Hat OpenStack Platform and OpenDaylight, with a specific focus on both the usage and deployment integration, and will run through an investigation of a pre-built environment, how to use it including some troubleshooting steps, and then an investigation of how it&#39;s possible to specify and configure OpenDaylight as part of your Red Hat OpenStack Platform deployment. We will use a combination of command-line tools and interaction via the OpenStack Dashboard (Horizon).</p>

<p>Whilst you&#39;ll be asked to use and explore some fundamental components within OpenStack, you won&#39;t need to install OpenStack or OpenDaylight from scratch within this lab, we&#39;ve provided a pre-installed OpenStack environment with OpenDaylight already integrated. The environment is comprised of one controller, one networker (where OpenDaylight runs), and two compute nodes. These machines will be virtual machines themselves, running on-top of a shared public cloud environment. You will have been provided with the necessary connection details on-screen and the first lab will demonstrate how to connect into the environment and how to get started with the lab sections.</p>

<blockquote>
<p><strong>NOTE</strong>: If you&#39;ve <strong>not</strong> been provided with connection details or you do not see your unique session information on-screen, please ask and we&#39;ll ensure that access is provided.</p>
</blockquote>

<p>If you have any problems at all or have any questions about Red Hat, our OpenStack distribution, or OpenDaylight, please put your hand-up and a lab moderator will be with you shortly to assist - we&#39;ve asked many of our OpenStack networking experts to be here today, so please make use of their time. If you have printed materials, they&#39;re yours to take away with you, otherwise the online copy will be available for the foreseeable future; I hope that they&#39;ll be useful assets in your OpenStack endeavours.</p>

<p>This lab is comprised of two major components, split into individual sections:</p>

<ul>
<li><strong>Overviews</strong> - Here we&#39;ll be mainly focusing on background information; a detailed and comprehensive look at the architecture/topology, components, features, and futures of OpenDaylight and its integration with OpenStack. For those that are already familiar with OpenDaylight concepts, you may want to skip over this and head to the hands-on labs to put things into practice.<br /><br /></li>
<li><strong>Hands-on Labs</strong> - Here we&#39;ll be providing some guided, hands-on labs that will walk you through the usage of an OpenDaylight-enabled OpenStack environment so that you can build your skills, experience, and expertise. This will be combined with an investigation into how OpenDaylight can be deployed by OSP director, Red Hat&#39;s deployment and lifecycle management tool for Red Hat OpenStack Platform.</li>
</ul>

<blockquote>
<p><strong>NOTE</strong>: This lab has been written specifically to coincide with the launch of Red Hat OpenStack Platform 12 (based on upstream Pike), where OpenDaylight integration will be at technology preview status.</p>
</blockquote>

<p><br /><br /></p>

<h2 id="toc_2"><strong>What is OpenDaylight?</strong></h2>

<p><center><img src="images/opendaylight.png" style="width: 600px;" border=0/></center></p>

<p><a href="https://www.opendaylight.org/" target="_blank">OpenDaylight</a>, hosted by the Linux Foundation, is an open-source, open-standards based Software Defined Networking (SDN) implementation; one that is flexible, modular, and scalable. Being modular, it&#39;s comprised of many different sub-components or projects that combine together to create a comprehensive solution that addresses many different use-cases and requirements. The OpenDaylight community is vibrant and ever expanding with a wide variety of ecosystem partners, enabling innovation and integration at all levels.</p>

<p>Customers are choosing to implement SDN&#39;s for a number of different reasons; the majority of which have nothing to do with OpenStack at all. Many organisations are desiring a much more comprehensive, robust, flexible, and centralised networking management platform at the core of their business, enabling them to have a greater degree of control over their networking implementation, reducing silos of networking infrastructure whilst enabling scalability. But on-top of this, a true SDN provides programmability of the networking fabric, and should not be restricted to a single area of the business; it should span all (often heterogenous) infrastructure providers, including physical hardware, virtual infrastructure, virtual switches and the physical networking layers. There should be a holistic view of the entire networking infrastructure, with management and intelligence provided through open API&#39;s and optional graphical representations.</p>

<p>There are a large number of vendors working in this space to provide their own SDN implementations, some of which are open-source, but many are proprietary based; examples include Juniper Contrail, Nuage Networks, VMware NSX, and Cisco ACI. OpenDaylight is a community project with the backing of many different vendors, Red Hat included, to create a comprehensive open-source alternative without vendor lock-in. Its goal is to further the adoption and innovation of software-defined networking through the creation of a common, industry supported platform; one that can adapt to the requirements of applications deployed on-top of it, rather than having applications constrained by classical networking constructs. OpenDaylight itself is quite closely aligned with NFV (Network Function Virtualisation) adoption due the similarities that NFV has on compute virtualisation with SDN and network virtualisation; SDN drives on-demand deployment of virtual network services when, and where, they are needed.</p>

<p>OpenDaylight has been adopted as the core SDN implementation of the <a href="https://www.opnfv.org/community/upstream-projects/opendaylight" target="_blank">OPNFV community</a>, as well as being positioned as a core-enabling technology for Telco/NFV customers for Red Hat. We have no current plans to productise OpenDaylight as a stand-alone product, but we&#39;ll be continuing to integrate it alongside Red Hat OpenStack Platform in the coming releases, where we&#39;ll be adding significant value to customers in these industry sectors with the features that they require for their implementations, such as comprehensive policy-driven control over networking, service-function chaining, and open-API&#39;s/standards for integration with existing datacentre investments. Red Hat will ensure that the components and modules that ship with OpenDaylight have been thoroughly tested for functionality, stability, and compatibility, ensuring that customers have an easy transition to an SDN environment should they want to.</p>

<p>The OpenDaylight architecture is modular and pluggable.  It consists of a Northbound layer (usually where REST API based applications exist), the Model-Driven Service Abstraction Layer (MD-SAL) which is used to service the core database as well as RPCs and communication between Services and Applications, as well as the Southbound layer, which contains many plug-ins used to program and interact with the network forwarding fabric. The following diagram picture shows a simplified view of the typical OpenDaylight architecture:
<br /><br /></p>

<p><center><img src="images/odl-architecture.png" style="width: 900px;" border=0/></center></p>

<p>The Red Hat OpenDaylight solution (part of the Red Hat OpenStack Platform) includes several Northbound plugins that provide things like authorisation (AAA), REST interface with Neutron (Neutron NB application), translating Neutron models to OpenFlow (NetVirt). NetVirt handles most of the complex logic of taking abstract Neutron network resources and translating it into OpenFlow/OVSDB based data models. In the future releases, more applications will be added to provide additional SDN/NFV functionality.</p>

<p>Most applications will only use a small subset of the available southbound plug-ins to control the data plane. The NetVirt application of the Red Hat OpenDaylight solution uses OpenFlow and Open vSwitch Database Management Protocol (OVSDB). The overview of the Red Hat OpenDaylight architecture is shown in the following diagram:
<br /><br /></p>

<p><center><img src="images/odl-parts.png" style="width: 600px;" border=0/></center>
<br /><br /></p>

<h2 id="toc_3"><strong>How does OpenDaylight integrate with OpenStack?</strong></h2>

<p>OpenStack Networking (Neutron) supports a plugin model that allows it to integrate with multiple different systems in order to implement networking capabilities for OpenStack. For the purpose of OpenStack integration, OpenDaylight exposes a single common northbound service, which is implemented by the Neutron Northbound component. The exposed API matches exactly the REST API of Neutron. This common service allows multiple Neutron providers to exist in OpenDaylight. As mentioned before, the Red Hat OpenDaylight solution is based on NetVirt as a Neutron provider for OpenStack. It is important to highlight that NetVirt consumes the Neutron API, rather than replacing or changing it.</p>

<p>The OpenDaylight plug-in for OpenStack Neutron is called <strong>networking-odl</strong>, and is responsible for passing the OpenStack network configuration into the OpenDaylight controller. The communication between OpenStack and OpenDaylight is done using the public REST APIs. This model simplifies the implementation on the OpenStack side, because it offloads most of the networking tasks onto OpenDaylight, which diminishes the processing burden for OpenStack.  OpenDaylight directly programs the Open vSwitch (OVS) switches in the data center.  This eliminates the need for most agents on every OpenStack node.  For example, when using OpenDaylight there is no OVS agent or L3 agent on compute. This means less processes running on compute nodes, which frees up more resources on a per node basis and allows greater scalability within OSP.  However, the Neutron DHCP agent is still used and exists on each OpenStack controller.
<br /><br /></p>

<p><center><img src="images/odl-neutron.png" style="width: 700px;" border=0/></center></p>

<p><br /><br />
The OpenDaylight controller uses NetVirt, then configures Open vSwitch instances (which use the OpenFlow and OVSDB protocols), and provides the necessary networking environment. This includes Layer 2 networking, L3 distributed virtual routing (DVR), security groups, and so on. The OpenDaylight controller can maintain the necessary isolation among different tenants. In addition, NetVirt is also able to control hardware gateways using the OVSDB protocol. A hardware gateway is typically a top of rack (ToR) Ethernet switch, that supports the OVSDB hardware_vtep schema, and can be used to connect virtual machines with physical devices. These physical machines may span over multiple L3 domains but still be able to function as part of the same broadcast L2 domain as the virtual machines thanks to VXLAN tunneling overlay, which terminates at the HW VTEP switch. The traffic is then mapped to VLANs and sent to the appropriate physical machine; making it seem as if the VMs and physical machines were in the same L2 domain.
<br /><br /></p>

<h2 id="toc_4"><strong>Overview of OpenDaylight Features with OpenStack (OSP12)</strong></h2>

<p>This section lists the key features available with OpenDaylight and Red Hat OpenStack Platform 12 release. Please skip ahead if you&#39;re already aware of these, or if you just want to have them as a reference for a later date.</p>

<ul>
<li><p><strong>Integration with Red Hat OpenStack Platform Director</strong> - The Red Hat OpenStack Platform director is a toolset for installing and managing a complete OpenStack environment. With Red Hat OpenStack Platform 12, director can deploy and configure OpenStack to work with OpenDaylight. OpenDaylight can run together with the OpenStack overcloud controller role, or as a separate custom role on a different node in several possible scenarios. In Red Hat Openstack Platform 12, OpenDaylight is installed and run in a <strong>container</strong> which provides more flexibility to its maintenance and use.</p></li>
<li><p><strong>L2 Connectivity between OpenStack instances</strong> - OpenDaylight provides the required Layer 2 (L2) connectivity among VM instances belonging to the same Neutron virtual network. Each time a Neutron network is created by a user, OpenDaylight automatically sets the required Open vSwitch (OVS) parameters on the relevant compute nodes to ensure that instances, belonging to the same network, can communicate with each other over a shared broadcast domain.</p>

<p>While VXLAN is the recommended encapsulation format for tenant networks traffic, 802.1q VLANs are also supported. In the case of VXLAN, OpenDaylight creates and manage the virtual tunnel endpoints (VTEPs) between the OVS nodes automatically to ensure efficient communication between the nodes, and without relying on any special features on the underlying fabric (the only requirement from the underlying network is support for unicast IP routing between the nodes).</p></li>
<li><p><strong>IP Address Management (IPAM)</strong> - VM instances get automatically assigned with an IPv4 address using the DHCP protocol, according to the tenant subnet configuration. This is currently done by leveraging the Neutron DHCP agent. Each tenant is completely isolated from other tenants, so that IP addresses can overlap.</p>

<blockquote>
<p><strong>NOTE</strong>: OpenDaylight can operate as a DHCP server. However, using the Neutron DHCP agent provides High Availability (HA) and support for VM instance metadata (cloud-init). Therefore Red Hat recommends to deploy the DHCP agent, rather than relying on OpenDaylight for such functionality at this time.</p>
</blockquote></li>
<li><p><strong>Routing between OpenStack networks</strong> - OpenDaylight provides support for Layer 3 (L3) routing between OpenStack networks, whenever a virtual router device is defined by the user. Routing is supported between different networks of the same project (tenant), which is also commonly referred to as East-West routing. OpenDaylight uses a distributed virtual routing paradigm, so that the packet forwarding and routing is done locally on each compute node and programmed directly into the switch dataplane.</p></li>
<li><p><strong>Floating IPs</strong> - A floating IP is a 1-to-1 IPv4 address mapping between a floating address (on an external network) and the fixed IP address, assigned to the instance in the tenant network. Once a VM instance is assigned with a floating IP by the user, the IP is used for any incoming or outgoing external communication. Red Hat OpenStack Platform director includes a default template, where each compute role has external connectivity for floating IPs communication. These external connections support both flat (untagged) and VLAN based networks, and is a source of <strong>DNAT</strong> (Destination Network Address Translation).</p></li>
<li><p><strong>Security Groups</strong> - OpenDaylight provides support for tenant configurable Security Groups that allow a tenant to control what traffic can flow in and out VM instances. Security Groups can be assigned per VM port or per Neutron network, and filter traffic based on TCP/IP characteristics such as IP address, IP protocol numbers, TCP/UDP port numbers and ICMP codes.</p>

<p>By default, each instance is assigned a default Security Group, where egress traffic is allowed, but all ingress traffic to the VM is blocked. The only exception is the trusted control-plane traffic such as ARP and DHCP. In addition, anti-spoofing rules are present, so a VM cannot send or receive packets with MAC or IP addresses that are unknown to Neutron. OpenDaylight also provides support for the Neutron port-security extension, that allows tenants to turn on or off security filtering on a per port basis. OpenDaylight implements the Security Groups rules within OVS in a stateful manner, by leveraging OpenFlow and conntrack.</p></li>
<li><p><strong>IPv6</strong> - IPv6 is an Internet Layer protocol for packet-switched networking and provides end-to-end datagram transmission across multiple IP networks, similarly to the previous implementation known as IPv4. The IPv6 networking not only offers far more IP addresses to connect various devices into the network, but it also allows to use other features that were previously not possible, such as stateless address autoconfiguration, network renumbering, and router announcements.</p>

<p>OpenDaylight in Red Hat OpenStack Platform 12 brings some feature parity in IPv6 use-cases with OpenStack Neutron. Some of the features that are supported in OpenDaylight include: IPv6 addressing support including stateless address autoconfiguration (SLAAC), DHCPv4 and DHCPv6 modes, IPv6 Security Groups along with allowed address pairs, IPv6 VM-to-VM communication in same network, IPv6 East-West routing, and Dual Stack (IPv4/IPv6) networks.</p></li>
<li><p><strong>VLAN aware VMs</strong> - VLAN aware VMs (or VMs with trunking support) allows an instance to be connected to one or more networks over one virtual NIC (vNIC). Multiple networks can be presented to an instance by connecting it to a single port. Network trunking lets users create a port, associate it with a trunk, and launch an instance on that port. Later, additional networks can be attached to or detached from the instance dynamically without interrupting the instance’s operations.</p>

<p>The trunk typically provides a parent port, which the trunk is associated with, and can have any number of child ports (subports). When users want to create instances, they need to specify the parent port of the trunk to attach the instance to it. The network presented by the subport is the network of the associated port. The VMs see the parent port as an untagged VLANs and the child ports are tagged VLANs.</p></li>
<li><p><strong>SNAT</strong> - SNAT (Source Network Address Translation) enables that virtual machines in a tenant network have access to the external network without using floating IPs. It uses NAPT (Network Address Port Translation) to allow multiple virtual machines communicating over the same router gateway to use the same external IP address.</p>

<p>Red Hat OpenStack Platform 12 introduces the conntrack based SNAT where it uses OVS netfilter integration where netfilter maintains the translations. One switch is designated as a NAPT switch, and performs the centralised translation role. All the other switches send the packet to centralised switch for SNAT. If a NAPT switch goes down an alternate switch is selected for the translations, but the existing translations will be lost on a failover.</p></li>
<li><p><strong>OVS-DPDK</strong> - Open vSwitch is a multilayer virtual switch that uses the OpenFlow protocol and its OVSDB interface to control the switch. The native Open vSwitch uses the kernel space to deliver data to the applications. The kernel creates the so called flow table which holds rules to forward the passing packets. Packets that do not match any rule, usually the first packets are sent to an application in the user space for further processing. When the application (a daemon) handles the packet, it makes a record in the flow table, so that next packets could use a faster path. Thus, OVS can save a reasonable amount of time by by-passing the time consuming switching between the kernel and the applications. Such approach can still have limitations in the bandwidth of the Linux network stack, which is unsuitable for use cases that require to process a high rate of packets, such as telecommunications.</p>

<p>DPDK is a set of user space libraries that enable a user to build applications that can process the data faster. It offers several Poll Mode Drivers (PMDs), that enable the packets to pass the kernel stack and go directly to the user space. Such behaviour speeds up the communication remarkably, because it handles the traffic outside of the kernel space completely. OpenDaylight in Red Hat Openstack Platform 12 may be deployed with Open vSwitch Data Plane Development Kit (DPDK) acceleration with director. This deployment offers higher data plane performance as packets are processed in user space rather than in the kernel.</p></li>
<li><p><strong>SR-IOV integration</strong> - The Single Root I/O Virtualisation (SR-IOV) specification is a standard for a type of PCI device assignment that can project a single networking device to multiple virtual machines and improve their performance. For example, SR-IOV enables a single Ethernet port to appear as multiple, separate, physical devices. A physical device with SR-IOV capabilities can be configured to appear in the PCI configuration space as multiple functions. Basically, SR-IOV distinguishes between Physical Functions (PFs) and Virtual Functions (VFs). PFs are full PCIe devices with SR-IOV capabilities. They provide the same functionality as usual PCI devices and can be assigned the VFs.</p>

<p>VFs are simple PCIe functions that derive from PFs. The number of Virtual Functions a device may have is limited by the device hardware. A single Ethernet port, the Physical Device, may map to many Virtual Functions that can be shared to virtual machines through the hypervisor. It maps one or more Virtual Functions to a virtual machine. Each VF can be mapped to a single guest at a time only, because it requires real hardware resources. A virtual machine can have more VFs. To the virtual machine, the VF appears as a usual networking card.</p>

<p>The main advantage is that the SR-IOV devices can share a single physical port with multiple virtual machines. Furthermore, the VFs have near-native performance and provide better performance than paravirtualised drivers and emulated access, and they provide data protection between virtual machines on the same physical server. OpenDaylight in Red Hat OpenStack Platform 12 can be deployed with compute nodes that support SR-IOV. It is also possible to create mixed environments with both OVS and SR-IOV nodes in a single OpenDaylight installation. The SR-IOV deployment requires the Neutron SR-IOV agent in order to configure the virtual functions (VFs), which are directly passed to the compute instance when it is deployed as a network port.</p></li>
<li><p><strong>Controller clustering</strong> - High availability is the continued availability of a service even when individual systems providing it fail. There are a number of different ways of implementing high availability; one desirable feature shared by most is that whatever operations are involved in ensuring continuity of service are handled automatically by the system, without administrator involvement. Typically system administrators will be notified when systems fail, but won’t need to take action to keep the overall service operational; they will only need to take manual action to restore the entire system to its nominal configuration.</p>

<p>The OpenDaylight Controller in Red Hat OpenStack Platform supports a cluster based High Availability model. Several instances of the OpenDaylight Controller form a Controller Cluster and together, they work as one logical controller. The service provided by the controller, viewed as a logical unit, continues to operate as long as a majority of the controller instances are functional and able to communicate with each other. The Red Hat OpenDaylight Clustering model provides both High Availability and horizontal scaling: more nodes can be added to absorb more load, if necessary.</p></li>
<li><p><strong>Hardware VXLAN VTEP (L2GW)</strong> - Layer 2 gateway services allow a tenant’s virtual network to be bridged to a physical network. This integration provides users with the capability to access resources on a physical server through a layer 2 network connection rather than via a routed layer 3 connection, that means extending the layer 2 broadcast domain instead of going through L3 or Floating IPs. To implement this, there is a need to create a bridge between the virtual workloads running inside an overlay (VXLAN) and workloads running in physical networks (normally using VLAN). This requires some sort of control over the physical top-of-rack (ToR) switch the physical workload is connected to. Hardware VXLAN Gateway (aka HW VTEP) can help with that.</p>

<p>HW VTEP (VXLAN Tunnel End Point) usually resides on the ToR switch itself and performs VXLAN encapsulation and de-encapsulation. Each VTEP device has two interfaces – one is a VLAN interface (facing the physical server) and the other is an IP interface to other VTEPs. The idea behind hardware VTEPs is to create an overlay network that connects VMs and physical servers and make them think that they’re in the same L2 network. Red Hat OpenStack customers can benefit from an L2GW to integrate traditional bare-metal services into a Neutron overlay. This is especially useful for bridging external physical workloads into a Neutron tenant network, BMaaS/Ironic for bringing a bare metal server (managed by OpenStack) into a tenant network, and bridging SR-IOV traffic into a VXLAN overlay; taking advantage of the line-rate speed of SR-IOV and the benefits of an overlay network to interconnect SR-IOV VMs.</p>

<h1 id="toc_5"><strong>Getting Started with the Labs</strong></h1></li>
</ul>

<p>The OpenStack environment that we&#39;re going to be using has been preinstalled and largely preconfigured (including OpenDaylight integration) for the purposes of this lab. We&#39;ve done our best to preconfigure the classroom and ensure that the cloud-based virtual machines that make up the infrastruture are ready to go at the start of the lab, but we need to ensure that you&#39;re able to log in to the environment, as the workstation you&#39;re at will be used for multiple different labs during the Red Hat Summit.</p>

<h1 id="toc_6">Lab Environment</h1>

<p>The environment you&#39;re going to be using, despite running in the public cloud, has been deployed with OSP director, Red Hat&#39;s deployment and lifecycle management platform for OpenStack. As such, it&#39;s been deployed with the TripleO methodology, i.e. a smaller &#39;bootstrap&#39; OpenStack cloud, known as the <strong>undercloud</strong> deploys the &#39;production&#39; cloud known as the <strong>overcloud</strong>, i.e. where your workloads would actually run. The overcloud that has been deployed in the lab environment is a simple configuration, comprising of a single controller, single networker, and dual compute node setup. All of these nodes, the undercloud, and all overcloud nodes are virtual machines running within a unique session on-top of the public-cloud ready for your consumption, roughly looking like the following:</p>

<p><img src="images/lab-setup.png" style="width: 1000px;"/></p>

<blockquote>
<p><strong>NOTE</strong>: In the above image you&#39;ll see the <strong>bastion</strong> host is the one that we&#39;ll be using as a jump host, i.e. the machine that we connect to first to establish a connection into the lab environment that we&#39;ll be using for the rest of the labs; this is the only virtual machine in your unique environment that has secure shell access exposed to the public internet.</p>
</blockquote>

<p>In this lab, you <strong>won&#39;t</strong> be utilising the undercloud platform to deploy any overcloud infrastructure - this has already been done for you, but you will be using it as the base in which we carry out our labs. If you&#39;re interested in learning more, there&#39;s a lab tomorrow, <strong>L1010</strong> (<a href="https://agenda.summit.redhat.com/SessionDetail.aspx?id=153520" target="_blank">Hands on with Red Hat OpenStack Platform director</a>) in the same room, which will allow you to configure and deploy OpenStack from scratch.</p>

<p>Using a virtualised infrastructure inside of the public cloud allows us to have full control over all of the network and storage without impacting other lab users, and whilst we won&#39;t be running any intensive workloads, it allows us to build up and test OpenStack in a short amount of time and with great flexibility. To re-iterate, we&#39;ll first be connecting to the <strong>jump host</strong>, and then further connecting to our <strong>undercloud</strong> machine as the conduit into our overcloud OpenStack environment. The undercloud will be used for both executing commands on the overcloud, and also as a conduit for connecting to our overcloud nodes and any deployed resources when required to do so.</p>

<p>A note on how these labs work - Note that when this lab guide asks you to execute a command, it will look like the following:</p>

<div><pre><code class="language-none">$ command-to-enter &lt;args&gt;
(output)</code></pre></div>

<p>Both the exact command to enter and the expected output will be shown (which may vary slightly within your environments, and may be trimmed down to reduce the size of the lab guide), so unless explicitly mentioned, you execute the steps described in such boxes. Please ask at any time if you require any assistance with this.</p>

<p><br /></p>

<h1 id="toc_7">Connecting to your Environment</h1>

<h1 id="toc_8">Lab Access</h1>

<p>We&#39;re using the Red Hat Product Demo Suite (RHPDS) for our labs, and therefore we need to request and get access to a unique environment based within the public cloud for you to use to complete the lab steps. If you&#39;re a Red Hat employee you&#39;ll need to follow these instructions to generate a session, othewise please get the connection details from your Red Hat representative and skip to the &#39;<strong>Connecting</strong>&#39; part below where we&#39;re connecting via secure-shell to the environment provided.</p>

<blockquote>
<p><strong>NOTE</strong>: Only proceed with the RHPDS creation instructions below if you&#39;re a Red Hat employee, or have been given access to RHPDS as a partner.</p>
</blockquote>

<p>First you&#39;ll need to request a session via RHPDS, the WebUI (and associated login page) can be found at <a href="https://rhpds.redhat.com/" target="_blank">https://rhpds.redhat.com/</a>. Once you&#39;ve logged in, navigate to the service catalogue by selecting &#39;<strong>Services</strong>&#39; --&gt; &#39;<strong>Catalogs</strong>&#39;, and navigate to the correct lab that you want to access by clicking &#39;<strong>Order</strong>&#39; on the right hand side. This lab is &#39;<strong>Hands on with OpenStack and OpenDaylight</strong>&#39; and should look like the following:</p>

<p><img src="images/order-odl1.png" style="width: 1000px;"/></p>

<p>Once you select &#39;Order&#39; you&#39;ll be presented with the following page which you&#39;ll need to accept some terms about the order time and the expiry:</p>

<p><img src="images/order-odl2.png" style="width: 1000px;"/></p>

<p>Select <strong>&#39;Submit&#39;</strong> at the bottom of the page and it should generate the environment for you, and will show up in your requests:</p>

<p><img src="images/requests.png" style="width: 1000px;"/></p>

<blockquote>
<p><strong>NOTE</strong>: This is a generic screenshot above, your output might look slightly different if you&#39;re using a different lab.</p>
</blockquote>

<p>The RHPDS system will now generate a unique environment for you to use and you will receive an email with some of the connection details. These details uniquely identify your session to ensure that you are connecting to your unique environment, see here for an example:</p>

<p><img src="images/email-odl.png" style="width: 1000px;"/></p>

<p>You&#39;ll notice that it contains some links, specifically the &quot;<strong>External Hostname</strong>&quot; for the <strong>WORKSTATION</strong> system - this is the <strong>jumphost</strong> that you&#39;ll be connecting to from the outside, and it has a unique hostname to connect to from the outside that&#39;s routable over the internet. Here, mine is <strong>&quot;odl-4f17.rhpds.opentlc.com&quot;</strong>. In addition, there are links to other areas such as the Horizon dashboard that you&#39;ll likely use later in the lab, as well as a link to these labs.</p>

<h1 id="toc_9">Connecting</h1>

<p>You&#39;ll see that my assigned lab UUID for my environment is &#39;<strong>4f17</strong>&#39; and is used to uniquely identify my session, and is used as part of the connection address. The environment takes around 20-30 minutes to power-up, and this should have already been done for you prior to the session starting, but don&#39;t be alarmed if you cannot connect in straight away, it may just require a few more minutes. Use the exact connection address that it provides you on your screen by copying and pasting the text from the webpage into a terminal emulator, here I&#39;m using my example but <strong>you&#39;ll need to replace this with your own username and unique session</strong>:</p>

<div><pre><code class="language-none">$ ssh odl-4f17.rhpds.opentlc.com -l (your RHPDS username)
The authenticity of host &#39;odl-4f17.rhpds.opentlc.com (129.146.91.32)&#39; can&#39;t be established.
ECDSA key fingerprint is SHA256:SqbVF0TGdHuTsoDChp6/cw4jFHqwJlBWFOeqwd88Bi4.
Are you sure you want to continue connecting (yes/no)? yes
(...)</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: The above assumes that you&#39;ve associated your public secure shell key with RHPDS - if you have not done so, please update it <a href="https://account.opentlc.com/update/" target="_blank">here</a>. If you have associated your key already then you&#39;re good to go and you shouldn&#39;t be required to use a password. <strong>If you have been assigned a system from a Red Hat employee, ensure he/she provides you with a username and keypair to use</strong>.</p>
</blockquote>

<p><br />
If successful, we can jump straight to our <strong>undercloud</strong> machine, as this is the one that we&#39;re going to be using for all of the lab sections, note that we&#39;re using sudo below as the root user on the jump host is the only one configured with the ssk-keys:</p>

<div><pre><code class="language-none">$ sudo ssh stack@undercloud</code></pre></div>

<p><strong>Only</strong> if this is unsuccessful (e.g. for some reason that there&#39;s no entry in /etc/hosts), attempt the following:</p>

<div><pre><code class="language-none">$ sudo ssh stack@192.168.122.253</code></pre></div>

<p><br />
Next, we can jump straight to our <strong>undercloud</strong> machine, as this is the one that we&#39;re going to be using for all of the lab sections, note that we&#39;re using sudo below as the root user on the jump host is the only one configured with the ssk-keys:</p>

<div><pre><code class="language-none">$ sudo ssh stack@undercloud</code></pre></div>

<p><strong>Only</strong> if this is unsuccessful (e.g. for some reason that there&#39;s no entry in /etc/hosts), attempt the following:</p>

<div><pre><code class="language-none">$ sudo ssh stack@192.168.122.253</code></pre></div>

<p>You will have full root access (via sudo) and control over this virtual machine, and we&#39;ll run our tasks directly here. If you&#39;re still unable to connect into your environment after a few minutes, please ask for assistance.</p>

<h2 id="toc_10"><strong>Lab Environment Exploration</strong></h2>

<p>In this section we&#39;re going to take a brief look of the pre-deployed lab environment to see how it has been put together. Unfortunately due to the time taken to complete a deployment end-to-end, the lab authors decided that working with a pre-deployed environment made the most sense; a deployment typically takes between 30-60 minutes on normal hardware (depending on the configuration), and when we tested within the public cloud environment it took much longer. Therefore we&#39;re not actually going to be deploying OpenStack in this lab, that step has already been done for you, but we&#39;ll investigate the integration of OpenDaylight and OpenStack over the next couple of hours, and will also demonstrate how it could have been configured through TripleO (OSP director) later on at the end.</p>

<p>The lab environment has been configured to look like the following-</p>

<p><img src="images/lab-environment.png" style="width: 1000px;"/>
There are six total machines, if you include the jump-host, that we&#39;ll be utilising. In a previous step we used the jump-host to connect into the undercloud (in <strong>blue</strong>), which is a small bootstrap OpenStack environment that&#39;s used to deploy the overcloud (in <strong>red</strong>). The overcloud has already been pre-deployed and already has OpenDaylight integrated, running on a dedicated &#39;<strong>networker</strong>&#39; node. Sitting on-top of all of these systems are dedicated VLANs that are used to segment OpenStack network traffic, e.g. ensuring that internal API communication is isolated from tenant network traffic, and so on.</p>

<p>Now that we&#39;ve successfully connected from the public internet to the jump-host, and have used that to get into our undercloud machine, let&#39;s explore what the current setup looks like a little further. All steps, unless explicitly mentioned, will be executed as the &#39;<strong>stack</strong>&#39; user on the undercloud machine; this is a non-privileged user account and will be used to perform all requirements of the lab sections - we will not need the root account, although you should have sudo access if you want to explore a bit further.</p>

<p>As highlighted previously, we&#39;re operating with two separate clouds here, both of which are running OpenStack; the <strong>undercloud</strong> and the <strong>overcloud</strong>; it&#39;s the undercloud that&#39;s used to <strong>bootstrap</strong> the overcloud. As such, getting access to each of these clouds requires the correct credentials and some parameters that specify where the relevant API&#39;s reside. Typically you&#39;d only ever need to interact with the undercloud if you were deploying, updating, or deleting an overcloud, but we&#39;re going to demonstrate the base functionality of the undercloud here just to level-set on the architecture of the overcloud.</p>

<h1 id="toc_11">The ~/stackrc and ~/overcloudrc files</h1>

<p>Upon initial installation of the undercloud (not covered in this lab), OSP director generates a file called &#39;<strong>stackrc</strong>&#39;; this file resides in the stack user&#39;s home directory and is a source of environment variables that enable the OpenStack command line tools to execute commands against the <strong>undercloud</strong> itself. Then, by default, after an <strong>overcloud</strong> has been deployed by the undercloud, OSP director (through TripleO) creates a file called <strong>overcloudrc</strong> (and <strong>overcloudrc.v3</strong>), which essentially provides very similar information to stackrc, but instead of telling the OpenStack command line tooling to point at the undercloud, it points it at the overcloud.</p>

<p>So, to re-iterate, the undercloud machine provides access to <strong>two</strong> functioning OpenStack deployments - one as the &quot;command and control&quot; or bootstrap cloud running on that system, and the other as the &quot;production&quot; OpenStack environment running within the overcloud nodes. Access is provided by either the &#39;stackrc&#39; file (for the undercloud), or &#39;overcloudrc&#39; file for the overcloud. The file you <strong>source</strong> will affect which deployment you&#39;re issuing commands to. Let&#39;s take a look at these files.</p>

<p>Assuming that you&#39;re still connected to your <strong>undercloud</strong> machine as the <strong>stack</strong> user,  you&#39;ll notice that these files are residing in stack&#39;s home directory, as we have both a functioning undercloud and overcloud. Let&#39;s first look at the stackrc file:</p>

<div><pre><code class="language-none">$ cat ~/stackrc

# Clear any old environment that may conflict.
for key in $( set | awk &#39;{FS=&quot;=&quot;}  /^OS_/ {print $1}&#39; ); do unset $key ; done
NOVA_VERSION=1.1
export NOVA_VERSION
OS_PASSWORD=$(sudo hiera admin_password)
export OS_PASSWORD
OS_AUTH_TYPE=password
export OS_AUTH_TYPE
OS_AUTH_URL=http://172.16.0.1:5000/
export OS_AUTH_URL
OS_USERNAME=admin
OS_PROJECT_NAME=admin
COMPUTE_API_VERSION=1.1
# 1.34 is the latest API version in Ironic Pike supported by ironicclient
IRONIC_API_VERSION=1.34
OS_BAREMETAL_API_VERSION=$IRONIC_API_VERSION
OS_NO_CACHE=True
OS_CLOUDNAME=undercloud
export OS_USERNAME
export OS_PROJECT_NAME
export COMPUTE_API_VERSION
export IRONIC_API_VERSION
export OS_BAREMETAL_API_VERSION
export OS_NO_CACHE
export OS_CLOUDNAME
OS_IDENTITY_API_VERSION=&#39;3&#39;
export OS_IDENTITY_API_VERSION
OS_PROJECT_DOMAIN_NAME=&#39;Default&#39;
export OS_PROJECT_DOMAIN_NAME
OS_USER_DOMAIN_NAME=&#39;Default&#39;
export OS_USER_DOMAIN_NAME

# Add OS_CLOUDNAME to PS1
if [ -z &quot;${CLOUDPROMPT_ENABLED:-}&quot; ]; then
    export PS1=${PS1:-&quot;&quot;}
    export PS1=\${OS_CLOUDNAME:+&quot;(\$OS_CLOUDNAME)&quot;}\ $PS1
    export CLOUDPROMPT_ENABLED=1
fi</code></pre></div>

<p>Here you can see that we set-up the <strong>username</strong>, <strong>password</strong>, and <strong>authentication URL</strong> that will give us everything we need to utilise the OpenStack client command line tools that you&#39;re no doubt already familiar with, but perhaps have never used them against a TripleO based OpenStack environment. There are also a large number of other environment variables that help set specific API versions to use, and make it clear which environment file has been sourced by overriding PS1.</p>

<p>So, let&#39;s make sure that our undercloud works, let&#39;s <strong>source</strong> this file (use this file as a source of environment variables):</p>

<div><pre><code class="language-none">$ source ~/stackrc</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: You&#39;ll note that your command line now shows &quot;(undercloud)&quot; before the command prompt to signify the cloud that you&#39;ll be executing commands against. But this isn&#39;t shown in the commands below.</p>
</blockquote>

<p>Now attempt to grab an authentication token from our undercloud:</p>

<div><pre><code class="language-none">$ openstack token issue
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field      | Value                                                                                                                                                                                   |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| expires    | 2018-04-17T00:09:02+0000                                                                                                                                                                |
| id         | gAAAAABa1QLeKwRwfspD-8WsK9SnwleYztJf9CF9WQyBzA4u37TQL2HRz1KW8N0aHqvL2WD6Y8MAmOvtc6QFkc-aLpizdDf-lUD3UEvvMAoZ5ir3hx5sCcpKh975D344qWhb2j_eAFbEXw0dO79tFXaQ15iC4jHgnqnYSvsyZl51_eUfK8LhDT4 |
| project_id | 4978efb1e94543c09196b23ca79e0443                                                                                                                                                        |
| user_id    | 7246b17734a242a29fa3bc2149af2b10                                                                                                                                                        |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: The token that is issued to you will be fully privileged - there&#39;s a key differentiation between the local Linux user (being a non-privileged account) and the user that we&#39;re authenticating as within OpenStack. This source file contains credentials for the <strong>&#39;admin&#39;</strong> user within the undercloud, and has no restrictions.</p>
</blockquote>

<p>The vast majority of the TripleO functionality is now built into the OpenStack client tools, and can typically be invoked by either &#39;<strong>openstack undercloud &lt;command&gt;</strong>&#39; or &#39;<strong>openstack overcloud &lt;command&gt;</strong>&#39;, depending on the cloud that needs to be actioned, although certain components may need to be interrogated individually.</p>

<p>As OSP director (via TripleO) controls bare-metal hardware to use as a base for the overcloud deployment, we can verify connectivity by asking it for the current list of hardware being used:</p>

<div><pre><code class="language-none">$ openstack baremetal node list
+--------------------------------------+--------------------+--------------------------------------+-------------+--------------------+-------------+
| UUID                                 | Name               | Instance UUID                        | Power State | Provisioning State | Maintenance |
+--------------------------------------+--------------------+--------------------------------------+-------------+--------------------+-------------+
| acfc2f77-adb2-4c25-b89e-8628d8debeae | summit-controller1 | b57eef5c-860c-43a3-9a66-77e6e592298f | power on    | active             | False       |
| a5c6c2ba-57d4-40c6-a3c6-86271d1479ff | summit-compute1    | 673c2bc2-8680-46c6-8b03-e3eeb3cff98f | power on    | active             | False       |
| 9622d2c1-947e-428f-a63e-e2d45c4a564d | summit-compute2    | 9d431a30-1738-421e-933b-5f357dce6fcc | power on    | active             | False       |
| 1d70281d-1fb7-4828-90c2-c0003f98eb7c | summit-networker1  | 68545239-2fac-4348-8968-d1e6d42d05e6 | power on    | active             | False       |
+--------------------------------------+--------------------+--------------------------------------+-------------+--------------------+-------------+</code></pre></div>

<p>Here you can see all of the overcloud nodes from the image we showed earlier on - all four overcloud machines with their power state as &#39;<strong>power on</strong>&#39; and a provision status as &#39;<strong>active</strong>&#39;, signifying that these systems are on, and being used - and we know they are, as they&#39;re our overcloud itself.</p>

<p>Now we&#39;re ready to move onto our overcloud itself; we won&#39;t need to worry about the <strong>&#39;stackrc&#39;</strong> file from now on as we&#39;re not going to be deploying OpenStack or making any changes to the existing configuration; using it was only to display how the undercloud is managing the hardware that makes up our overcloud. From now on we&#39;ll use the overcloudrc file, as it will allow us to interact with, and investigate the pre-deployed overcloud environment. Getting access to that environment is as easy as sourcing the overcloudrc file:</p>

<div><pre><code class="language-none">$ source ~/overcloudrc</code></pre></div>

<p>Now, instead of the commands going to the undercloud, the commands will go to the <strong>overcloud</strong>, or more specifically, the OpenStack controller(s) running there. You should also notice that the command prompt will have updated to demonstrate that we&#39;re interactiving with the overcloud.</p>

<p>Now, let&#39;s briefly list out the current set of services that are offered by the overcloud; this will allow us to verify connectivity to our overcloud, and that our overcloudrc file is providing us with everything we need to do so:</p>

<div><pre><code class="language-none">$ openstack service list
+----------------------------------+------------+----------------+
| ID                               | Name       | Type           |
+----------------------------------+------------+----------------+
| 011cbcb8ec59472f986226fe42ee4a09 | placement  | placement      |
| 1af179a63e3f4411b080accd8c4eae77 | cinderv3   | volumev3       |
| 25e3ec7de8ea4c498aed1d90684104b1 | keystone   | identity       |
| 2b8e5d7341014ede93e28c0366205868 | heat-cfn   | cloudformation |
| 36733da763024e6dbcc8d700f9644b85 | gnocchi    | metric         |
| 6106a390fe994022bd904152832ea45c | ceilometer | metering       |
| 6bba6fea02b54bacb54c68a5379032b0 | cinderv2   | volumev2       |
| 7008b8e0bbfc42bd8132d684e3adc749 | neutron    | network        |
| 9d39cc71feb54138af6c061198f5f94c | nova       | compute        |
| b0c67504cfad4322ae797451dbabc567 | aodh       | alarming       |
| ba933279a0a34d399a6e2d36f68212f8 | cinder     | volume         |
| c3369e4a285d469794e1ae231cb4fa4f | heat       | orchestration  |
| c51021ec815c44a08fa3aa4370dd4622 | panko      | event          |
| d83434146a8c497d8dc5525d8e9ccf92 | glance     | image          |
| dc7cc4f3d155495caced459ef120c8e7 | swift      | object-store   |
+----------------------------------+------------+----------------+</code></pre></div>

<p>And one way to absolutely verify that we&#39;re talking to the overcloud and not the overcloud is to check on the endpoints for a given service. Our fixed virtual IP for our OpenStack services on the external network (routable from the undercloud) is <strong>192.168.122.100</strong>:</p>

<div><pre><code class="language-none">$ openstack endpoint show neutron
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| adminurl     | http://172.17.1.15:9696          |
| enabled      | True                             |
| id           | 9359b04363a14d1987fbc4f8fed643b9 |
| internalurl  | http://172.17.1.15:9696          |
| publicurl    | http://192.168.122.100:9696      |           &lt;--- see here.
| region       | regionOne                        |
| service_id   | 7008b8e0bbfc42bd8132d684e3adc749 |
| service_name | neutron                          |
| service_type | network                          |
+--------------+----------------------------------+</code></pre></div>

<h1 id="toc_12"><strong>Extra Files</strong></h1>

<p>In addition to these files you&#39;ll find a &#39;<strong>labs</strong>&#39; directory within the stack user&#39;s home directory. This will contain a number of different files that we&#39;ll be using over the next few lab sections, including all of the OSP director templates that were used for deployment. As we have a number of different OpenStack labs going on during the Red Hat Summit that our team have prepared, the same shared directory is available on all systems:</p>

<div><pre><code class="language-none">$ ls -l ~/labs
total 564048
-rw-rw-r--. 1 stack stack  13267968 Feb 10  2017 cirros-0.3.5-x86_64-disk.img
drwxrwxr-x. 2 stack stack        97 Mar 26 01:20 config
drwxrwxr-x. 3 stack stack        71 Mar 25 20:07 director
-rw-r--r--. 1 stack stack      1071 Mar 25 20:07 instackenv.json
drwxrwxr-x. 3 stack stack        77 Mar 25 20:07 odl
drwxrwxr-x. 3 stack stack       172 Mar 25 20:07 osp
-rw-rw-r--. 1 stack stack 564330496 Mar 22 07:38 rhel-server-7.4-x86_64-kvm.qcow2</code></pre></div>

<p>Don&#39;t worry about exploring for now, just know that we have these available for us.</p>

<h2 id="toc_13"><strong>Testing, Investigating, and Using ODL</strong></h2>

<p>So far, all we&#39;ve done is take a basic look at what the architecture looks like; we&#39;ve not deployed any resources, we&#39;ve not looked into the OpenDaylight integration, and we certainly don&#39;t know if it&#39;s even working yet. Right now you&#39;re going to have to trust us that it has been configured properly, but we&#39;ll start testing that right away.</p>

<p>In this section we&#39;re going to be verifying the environment to ensure that it first of all functions properly, but secondly adheres to a well-integrated OpenDaylight setup. The first thing we should do is make sure that OpenStack itself is working; let&#39;s build out some resources within our overcloud environment.</p>

<h3 id="toc_14">Uploading an Image to Glance</h3>

<p>As part of the lab, we&#39;ll use an image already residing on the filesystem for you; this is a stripped down version of RHEL 7.4 with a pre-set root password; we&#39;ll upload it as our own. Let&#39;s verify that the disk image is as expected and has the correct properties:</p>

<div><pre><code class="language-none">$ qemu-img info ~/labs/rhel-server-7.4-x86_64-kvm.qcow2
image: /home/stack/labs/rhel-server-7.4-x86_64-kvm.qcow2
file format: qcow2
virtual size: 10G (10737418240 bytes)
disk size: 538M
cluster_size: 65536
Format specific information:
    compat: 0.10
    refcount bits: 16</code></pre></div>

<p>Next we can create a new image within Glance and import its contents, it may take a few minutes to copy the data. Let&#39;s ensure that we&#39;ve sourced our <strong>overcloudrc</strong> file (noting that it doesn&#39;t matter if you&#39;ve already sourced this file - repeating the source command is safe), and proceed with the image creation:</p>

<div><pre><code class="language-none">$ source ~/overcloudrc

$ openstack image create rhel7 --public \
    --disk-format qcow2 --container-format bare \
    --file ~/labs/rhel-server-7.4-x86_64-kvm.qcow2

+------------------+------------------------------------------------------------------------------+
| Field            | Value                                                                        |
+------------------+------------------------------------------------------------------------------+
| checksum         | 2065a01cacd127c2b5f23b1738113325                                             |
| container_format | bare                                                                         |
| created_at       | 2018-04-16T20:49:39Z                                                         |
| disk_format      | qcow2                                                                        |
| file             | /v2/images/2650782f-e95c-4309-b041-49f79468413d/file                         |
| id               | 2650782f-e95c-4309-b041-49f79468413d                                         |
| min_disk         | 0                                                                            |
| min_ram          | 0                                                                            |
| name             | rhel7                                                                        |
| owner            | c14b205e428e43319fe43fb0396bd092                                             |
| properties       | direct_url=&#39;swift+config://ref1/glance/2650782f-e95c-4309-b041-49f79468413d&#39; |
| protected        | False                                                                        |
| schema           | /v2/schemas/image                                                            |
| size             | 564330496                                                                    |
| status           | active                                                                       |
| tags             |                                                                              |
| updated_at       | 2018-04-16T20:49:49Z                                                         |
| virtual_size     | None                                                                         |
| visibility       | public                                                                       |
+------------------+------------------------------------------------------------------------------+</code></pre></div>

<p>This may take a minute or so, but you can verify that the image is <strong>active</strong> and ready:</p>

<div><pre><code class="language-none">$ openstack image list
+--------------------------------------+-------+--------+
| ID                                   | Name  | Status |
+--------------------------------------+-------+--------+
| 2650782f-e95c-4309-b041-49f79468413d | rhel7 | active |
+--------------------------------------+-------+--------+</code></pre></div>

<h3 id="toc_15">Creating a Flavor to use</h3>

<p>Next we&#39;re going to need a flavor for our environment; as we&#39;re relatively resource constrained the out of the box flavors don&#39;t quite give us what we need, so let&#39;s create an additional flavor for us to use:</p>

<div><pre><code class="language-none">$ openstack flavor create --ram 2048 --disk 10 --vcpus 2 --id 6 m1.labs
+----------------------------+---------+
| Field                      | Value   |
+----------------------------+---------+
| OS-FLV-DISABLED:disabled   | False   |
| OS-FLV-EXT-DATA:ephemeral  | 0       |
| disk                       | 10      |
| id                         | 6       |
| name                       | m1.labs |
| os-flavor-access:is_public | True    |
| properties                 |         |
| ram                        | 2048    |
| rxtx_factor                | 1.0     |
| swap                       |         |
| vcpus                      | 2       |
+----------------------------+---------+</code></pre></div>

<h3 id="toc_16">Networking Configuration</h3>

<p>A little bit of background given that this is a networking lab...</p>

<p>When it comes to OpenStack Networking there are many different types of networks to consider; first you have the <strong>infrastructure</strong> networks where OpenStack services communicate across, where storage data is transferred, and how administrators connect to the machines, and then you have the <strong>instance</strong> networks, i.e. those networks that users can attach their workloads onto. These networks also come in all different shapes and sizes, depending on the use-case, e.g. whether instances are directly connected to an existing datacentre network (e.g. <strong>provider</strong> networks), whether they use high performance configurations such as SR-IOV or DPDK, or whether they use the default <strong>tenant</strong> networking model.</p>

<p>Tenant networks provide fully granular control of networking inside of each tenant, i.e. each tenant can create networks, have control over the subnet allocation, and provide additional services such as DHCP without requiring administrative access. Tenant networks are isolated from one another using either overlay technologies such as VXLAN, or via traditional isolation mechanisms such as VLAN, with the assignment and configuration of each type being automated by the OpenStack components. The problem with tenant networks is that they are inherently isolated networks that have no outbound or inbound connectivity - they’re designed to provide networking access between instances residing within said tenant.</p>

<p>To provide routing both ingress and egress to that tenant network requires the attachment of a virtual router, controlled by the chosen OpenStack Networking plugin (in our case OpenDaylight) where said router will, by default, provide SNAT capabilities to allow egress traffic. On-top of this, the virtual router can provide DNAT capabilities for ingress traffic through the concept of a floating-IP, which is attached on a 1:1 basis to an instance, allowing NAT based communication from an external routed network into the tenant network.</p>

<p>In a vanilla OpenStack configuration (e.g. one that uses ML2/OVS), this routing mechanism takes place on either dedicated networker nodes, or via the OpenStack controller nodes themselves in a centralised configuration, which can cause potential bottlenecks in performance as all North/South traffic goes through a centralised set of nodes. But with OpenDaylight, the responsibility for DNAT/SNAT resides with the compute node hosting said virtual machine.  SNAT is implemented by using conntrack (part of Linux Netfilter suite) to track connections and then Netfilter entries handle NAT translation.</p>

<p>We&#39;re going to use the default <strong>tenant</strong> network model in this lab with distributed routing on our compute nodes through an <strong>external</strong> network that we&#39;re going to define. We&#39;ll create these networks here and investigate how they&#39;re constructed and enabled through OpenDaylight later on. We&#39;re using the provider network extension to advise Neutron on the logical network mapping (i.e. how the external network is physically attached on the underlying nodes); we&#39;ll explore this later too.</p>

<p>First define the external network:</p>

<div><pre><code class="language-none">$ openstack network create external --external \
--provider-physical-network datacentre \
--provider-network-type flat

+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-04-18T12:52:01Z                 |
| description               |                                      |
| dns_domain                | None                                 |
| id                        | 642f4496-7773-4d17-8f82-52fe8efc2a62 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1500                                 |
| name                      | external                             |
| port_security_enabled     | True                                 |
| project_id                | c14b205e428e43319fe43fb0396bd092     |
| provider:network_type     | flat                                 |  &lt;--- Note the &#39;flat&#39; type
| provider:physical_network | datacentre                           |  &lt;--- Note the &#39;datacentre&#39; mapping
| provider:segmentation_id  | None                                 |
| qos_policy_id             | None                                 |
| revision_number           | 4                                    |
| router:external           | External                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-04-09T14:10:16Z                 |
+---------------------------+--------------------------------------+    </code></pre></div>

<p>There are a number of key parameters here, for reference use the table below:</p>

<table>
<thead>
<tr>
<th>Parameter</th>
<th>Details</th>
</tr>
</thead>

<tbody>
<tr>
<td>--provider-physical-network</td>
<td>This defines the <strong>logical</strong> physical network name that OpenDaylight uses to map this virtual network to a physical network on the compute host</td>
</tr>
<tr>
<td>--provider-network-type</td>
<td>This sets the network type, e.g. if it&#39;s flat, or VLAN tagged</td>
</tr>
<tr>
<td>--external</td>
<td>This tells Neutron that this is an <strong>external</strong> network and can be used for routing (e.g. floating IP&#39;s)</td>
</tr>
</tbody>
</table>

<p>Let&#39;s associate a subnet to our external network so that Neutron knows what IP ranges to use for outbound routing (SNAT) and inbound routing via floating IP&#39;s (DNAT), noting that this coincides with the network we have routing access from our undercloud machine:</p>

<div><pre><code class="language-none">$ openstack subnet create external_subnet --network external \
--subnet-range 192.168.122.0/24 \
--allocation-pool start=192.168.122.200,end=192.168.122.249 \
--no-dhcp --dns-nameserver 192.168.122.1 --gateway 192.168.122.1

+-------------------------+--------------------------------------+
| Field                   | Value                                |
+-------------------------+--------------------------------------+
| allocation_pools        | 192.168.122.200-192.168.122.249      |
| cidr                    | 192.168.122.0/24                     |
| created_at              | 2018-04-18T12:52:05Z                 |
| description             |                                      |
| dns_nameservers         | 192.168.122.1                        |
| enable_dhcp             | False                                |
| gateway_ip              | 192.168.122.1                        |
| host_routes             |                                      |
| id                      | f771130a-227d-4945-b836-8f80f970a20f |
| ip_version              | 4                                    |
| ipv6_address_mode       | None                                 |
| ipv6_ra_mode            | None                                 |
| name                    | external_subnet                      |
| network_id              | 642f4496-7773-4d17-8f82-52fe8efc2a62 |
| project_id              | c14b205e428e43319fe43fb0396bd092     |
| revision_number         | 0                                    |
| segment_id              | None                                 |
| service_types           |                                      |
| subnetpool_id           | None                                 |
| tags                    |                                      |
| updated_at              | 2018-04-18T12:52:05Z                 |
| use_default_subnet_pool | None                                 |
+-------------------------+--------------------------------------+</code></pre></div>

<p>As previously, there are a number of key parameters here, for reference use the table below:</p>

<table>
<thead>
<tr>
<th>Parameter</th>
<th>Details</th>
</tr>
</thead>

<tbody>
<tr>
<td>--subnet-range</td>
<td>This defines the CIDR of the network that we&#39;re wanting to create</td>
</tr>
<tr>
<td>--allocation-pool</td>
<td>This sets the range of IP&#39;s within the CIDR that we can use to allocate as floating IP&#39;s, or to use as SNAT IP&#39;s</td>
</tr>
<tr>
<td>--no-dhcp</td>
<td>This disables any form of DHCP service for this network - this is an external network that we use for bridging traffic between internal and external networks. In this lab, instances cannot be directly connected to the external network without routing (NAT) taking place via the L3 agent, although it is possible to establish this type of alternative configuration. Therefore, as instances won&#39;t be DHCP&#39;ing on this network, we disable this functionality.</td>
</tr>
<tr>
<td>--dns-nameserver</td>
<td>This defines the nameserver that instances can use, although this is not required as it&#39;s provided only when DHCP is enabled. It&#39;s shown here for completeness.</td>
</tr>
<tr>
<td>--gateway</td>
<td>This defines the upstream network gateway for the external network, i.e. the next-hop that would be used within that external network.</td>
</tr>
</tbody>
</table>

<p>Now that we&#39;ve got the external network configured we need to create <strong>internal</strong> networks for our instances as they won&#39;t have direct access to this network. This is the responsibility of a user within a project, the external network is just exposed to all of the projects that are created; whilst they cannot modify it, they can attach a virtual router to it for routing and floating IP access.</p>

<p>If we create a network and don&#39;t specify any additional parameters, Neutron assumes that it&#39;s a private tenant network that uses some form of network isolation mechanism (to isolate tenant networks between other projects) such as VLAN, or VXLAN. In our environment we&#39;re defaulting to <strong>VXLAN</strong>:</p>

<div><pre><code class="language-none">$ openstack network create internal
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-04-18T13:13:02Z                 |
| description               |                                      |
| dns_domain                | None                                 |
| id                        | 22f9a9cd-1da0-4c6a-a815-d25da07aa18d |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | internal                             |
| port_security_enabled     | True                                 |
| project_id                | c14b205e428e43319fe43fb0396bd092     |
| provider:network_type     | vxlan                                |   &lt;--- Note the &#39;vxlan&#39; type
| provider:physical_network | None                                 |
| provider:segmentation_id  | 3                                    |
| qos_policy_id             | None                                 |
| revision_number           | 3                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-04-09T14:11:53Z                 |
+---------------------------+--------------------------------------+</code></pre></div>

<blockquote>
<p><strong>NOTE:</strong> As we&#39;re using VXLAN, it&#39;s assumed that we&#39;re using the default frame size of 1500bytes, and has automatically reduced the MTU given to instances to 1450 to accomodate the VXLAN header.</p>
</blockquote>

<p>Next, associate a subnet with this network as before, but using a completely different subnet CIDR:</p>

<div><pre><code class="language-none">$ openstack subnet create internal_subnet --network internal \
    --subnet-range 172.16.1.0/24 --dns-nameserver 192.168.122.1

+-------------------------+--------------------------------------+
| Field                   | Value                                |
+-------------------------+--------------------------------------+
| allocation_pools        | 172.16.1.2-172.16.1.254              |
| cidr                    | 172.16.1.0/24                        |
| created_at              | 2018-04-18T13:13:09Z                 |
| description             |                                      |
| dns_nameservers         | 192.168.122.1                        |
| enable_dhcp             | True                                 |
| gateway_ip              | 172.16.1.1                           |
| host_routes             |                                      |
| id                      | 8146d9b3-8c97-436c-88df-3b3cd88ec54f |
| ip_version              | 4                                    |
| ipv6_address_mode       | None                                 |
| ipv6_ra_mode            | None                                 |
| name                    | internal_subnet                      |
| network_id              | 22f9a9cd-1da0-4c6a-a815-d25da07aa18d |
| project_id              | c14b205e428e43319fe43fb0396bd092     |
| revision_number         | 0                                    |
| segment_id              | None                                 |
| service_types           |                                      |
| subnetpool_id           | None                                 |
| tags                    |                                      |
| updated_at              | 2018-04-18T13:13:09Z                 |
| use_default_subnet_pool | None                                 |
+-------------------------+--------------------------------------+  </code></pre></div>

<p>This means that any instances that we start on this internal network will receive an IP address (via DHCP) on the <strong>172.16.1.0/24</strong> network, noting that by default, Neutron assumes that you want to have DHCP enabled, and will automatically assign an address to use for the default gateway (this will be the address that the L3 agent uses as a gateway for NAT).</p>

<p>At the moment, any instances attached to this network will be completely <strong>isolated</strong>; there&#39;s no routing between the internal and the external network - despite having a gateway defined we cannot get out, and we cannot get in. Neutron allows you to bridge tenant networks and external networks via virtual <strong>routers</strong>, and is what gets implemented by the OpenDaylight in a distributed function to perform such capabilities. Let&#39;s create a virtual router for our network:</p>

<div><pre><code class="language-none">$ openstack router create demo_router
+-------------------------+--------------------------------------+
| Field                   | Value                                |
+-------------------------+--------------------------------------+
| admin_state_up          | UP                                   |
| availability_zone_hints | None                                 |
| availability_zones      | None                                 |
| created_at              | 2018-04-18T13:13:18Z                 |
| description             |                                      |
| distributed             | False                                |
| external_gateway_info   | None                                 |
| flavor_id               | None                                 |
| ha                      | False                                |
| id                      | fa88e28a-34c4-4b8e-8c33-b67e54c4c9c0 |
| name                    | demo_router                          |
| project_id              | c14b205e428e43319fe43fb0396bd092     |
| revision_number         | None                                 |
| routes                  |                                      |
| status                  | ACTIVE                               |
| tags                    |                                      |
| updated_at              | 2018-04-18T13:13:18Z                 |
+-------------------------+--------------------------------------+  </code></pre></div>

<p>Now let&#39;s associate a gateway for this router, in other words, to which external network do we want outbound traffic to route through? That&#39;ll be our &#39;<strong>external</strong>&#39; network...</p>

<div><pre><code class="language-none">$ openstack router set demo_router --external-gateway external</code></pre></div>

<p>Then we need to add an interface to the internal network that we created in a previous step, noting that we need to associate it to the subnet, and not the network (as a network may have multiple subnets associated with it):</p>

<div><pre><code class="language-none">$ openstack router add subnet demo_router internal_subnet</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: The above two commands produce no output unless there have been any errors.</p>
</blockquote>

<p>In conclusion, we&#39;ve created a private &#39;tenant network&#39;, called <strong>&#39;internal&#39;</strong> for our instances to use and have created a virtual router for our internal network to route traffic to the outside; onto the administratively defined external network called <strong>&quot;external&quot;</strong>.</p>

<h3 id="toc_17">Instance Boot up</h3>

<p>Next, boot a new instance on OpenStack using our new flavor, the <strong>internal</strong> tenant network, and the image we uploaded earlier (&quot;<strong>rhel7</strong>&quot;), ensuring that you specify a name for the instance, below we use &quot;<strong>my_vm</strong>&quot;:</p>

<div><pre><code class="language-none">$ openstack server create --flavor m1.labs --image rhel7 --network internal my_vm
+-------------------------------------+----------------------------------------------+
| Field                               | Value                                        |
+-------------------------------------+----------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                       |
| OS-EXT-AZ:availability_zone         |                                              |
| OS-EXT-SRV-ATTR:host                | None                                         |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                         |
| OS-EXT-SRV-ATTR:instance_name       |                                              |
| OS-EXT-STS:power_state              | NOSTATE                                      |
| OS-EXT-STS:task_state               | scheduling                                   |
| OS-EXT-STS:vm_state                 | building                                     |
| OS-SRV-USG:launched_at              | None                                         |
| OS-SRV-USG:terminated_at            | None                                         |
| accessIPv4                          |                                              |
| accessIPv6                          |                                              |
| addresses                           |                                              |
| adminPass                           | X5txyyS2yZDy                                 |
| config_drive                        |                                              |
| created                             | 2018-04-18T13:20:15Z                         |
| flavor                              | m1.labs (6)                                  |
| hostId                              |                                              |
| id                                  | 5d24c6de-2d4d-42d6-bcd0-52dd2770de69         |
| image                               | rhel7 (81ff217a-38c5-4275-9ad4-f77a10e2a08b) |
| key_name                            | None                                         |
| name                                | my_vm                                        |
| progress                            | 0                                            |
| project_id                          | c14b205e428e43319fe43fb0396bd092             |
| properties                          |                                              |
| security_groups                     | name=&#39;default&#39;                               |
| status                              | BUILD                                        |
| updated                             | 2018-04-18T13:20:15Z                         |
| user_id                             | 1f474841c896452592072710e97d9ddc             |
| volumes_attached                    |                                              |
+-------------------------------------+----------------------------------------------+</code></pre></div>

<p>We can verify that our system has been started successfully with the following command, noting that it may take a few minutes to become active:</p>

<div><pre><code class="language-none">$ openstack server list
+--------------------------------------+-------+--------+---------------------+-------+---------+
| ID                                   | Name  | Status | Networks            | Image | Flavor  |
+--------------------------------------+-------+--------+---------------------+-------+---------+
| 5d24c6de-2d4d-42d6-bcd0-52dd2770de69 | my_vm | ACTIVE | internal=172.16.1.7 | rhel7 | m1.labs |
+--------------------------------------+-------+--------+---------------------+-------+---------+</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: You may see that the system is in a <strong>&quot;SPAWNING&quot;</strong> state for a few minutes, this is to be expected as the machine provisions itself. Please be patient and allow it to start. We want to make sure that it goes into an <strong>&quot;ACTIVE&quot;</strong> state.</p>
</blockquote>

<h3 id="toc_18"><strong>Creating and Assigning Floating IP&#39;s</strong></h3>

<p>As highlighted above, our launched instance is only on the internal network and has no connectivity from the outside world. We&#39;ll request a floating IP for our instance on the external network:</p>

<div><pre><code class="language-none">$ openstack floating ip create external
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| created_at          | 2018-04-18T13:35:49Z                 |
| description         |                                      |
| fixed_ip_address    | None                                 |
| floating_ip_address | 192.168.122.200                      |
| floating_network_id | 642f4496-7773-4d17-8f82-52fe8efc2a62 |
| id                  | 4965c561-c97e-48c0-b89e-7f9064d399f6 |
| name                | 192.168.122.200                      |
| port_id             | None                                 |
| project_id          | c14b205e428e43319fe43fb0396bd092     |
| revision_number     | 0                                    |
| router_id           | None                                 |
| status              | DOWN                                 |
| updated_at          | 2018-04-18T13:35:49Z                 |
+---------------------+--------------------------------------+  </code></pre></div>

<p>You can see that it&#39;s reserved <strong>192.168.122.200</strong> for our project, although it&#39;s not attached to an instance <em>yet</em> and therefore is not much use. Next, we can assign our claimed IP address to an instance:</p>

<div><pre><code class="language-none">$ openstack server add floating ip my_vm 192.168.122.200    </code></pre></div>

<blockquote>
<p><strong>NOTE:</strong> If the command is successful it has no output, and your IP address may vary from the one displayed above - use the IP address that the create command allocated to you.</p>
</blockquote>

<p>You can now verify that the IP address was assigned to your node with the following:</p>

<div><pre><code class="language-none">$ openstack server list
+--------------------------------------+-------+--------+--------------------------------------+-------+---------+
| ID                                   | Name  | Status | Networks                             | Image | Flavor  |
+--------------------------------------+-------+--------+--------------------------------------+-------+---------+
| 5d24c6de-2d4d-42d6-bcd0-52dd2770de69 | my_vm | ACTIVE | internal=172.16.1.7, 192.168.122.200 | rhel7 | m1.labs |
+--------------------------------------+-------+--------+--------------------------------------+-------+---------+</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: It may take a while for the floating IP to show up here, so keep trying the above command. </p>
</blockquote>

<p>For those of you that may have already tried to ping or SSH into the node using this IP address may be confused as to why this is not working. By default, OpenStack&#39;s security group rules will prevent <strong>all</strong> inbound access, so we&#39;ll need to open these up before we confirm that our instance is working properly.</p>

<h3 id="toc_19"><strong>OpenStack Security Groups</strong></h3>

<p>By default, OpenStack Security Groups prevent any inbound access to instances, including ICMP/ping. Therefore, we have to edit the security group policy to ensure that the firewall is opened up for us.</p>

<p>Let&#39;s add two rules, firstly for all instances to have ICMP and SSH access. By default, Neutron ships with a &#39;default&#39; security group, it&#39;s possible to create new groups and assign custom rules to these groups and then assign these groups to individual servers. For this lab, we&#39;ll just configure the default group. The problem is that there are multiple default groups, and as an administrator you can see them all, noting that they&#39;re project-specific. We need to first connect the <strong>admin</strong> project to the correct security group, first by getting the correct project ID:</p>

<div><pre><code class="language-none">$ export MY_PROJECT=$(openstack project list | awk &#39;$4 == &quot;admin&quot; {print $2};&#39;)
$ export SEC_GROUP_ID=$(openstack security group list | grep $MY_PROJECT | awk &#39;{print $2;}&#39;)</code></pre></div>

<p>This should now show you the correct security group ID:</p>

<div><pre><code class="language-none">$ echo $SEC_GROUP_ID
2641d918-579b-4acd-8fbf-894f4e7be241 

$ openstack security group list -c ID -c Project
+--------------------------------------+----------------------------------+
| ID                                   | Project                          |
+--------------------------------------+----------------------------------+
| 2641d918-579b-4acd-8fbf-894f4e7be241 | c14b205e428e43319fe43fb0396bd092 |
| 9d1ac5cf-52da-4de7-a082-f10e7db88c71 | b86d79c73e604feab8ced0a46fe9738b |
| bf70f47e-4c58-4372-81e2-5d38e14e3ec6 |                                  |
+--------------------------------------+----------------------------------+</code></pre></div>

<p>So, let&#39;s look at enabling ICMP within the default group, using the security group ID as the unique identifier for our &#39;<strong>default</strong>&#39; group within the admin project that we&#39;re using (and that our instance is booted onto):</p>

<div><pre><code class="language-none">$ openstack security group rule create --proto icmp $SEC_GROUP_ID
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| created_at        | 2018-04-18T13:59:15Z                 |
| description       |                                      |
| direction         | ingress                              |
| ether_type        | IPv4                                 |
| id                | 8671ae28-b52a-4afa-b02b-2a632d55daf6 |
| name              | None                                 |
| port_range_max    | None                                 |
| port_range_min    | None                                 |
| project_id        | c14b205e428e43319fe43fb0396bd092     |
| protocol          | icmp                                 |
| remote_group_id   | None                                 |
| remote_ip_prefix  | 0.0.0.0/0                            |
| revision_number   | 0                                    |
| security_group_id | 2641d918-579b-4acd-8fbf-894f4e7be241 |
| updated_at        | 2018-04-18T13:59:15Z                 |
+-------------------+--------------------------------------+</code></pre></div>

<p>Within a few seconds (for the hypervisor to pick up the changes) you should be able to ping your floating IP:</p>

<div><pre><code class="language-none">$ $ ping -c4 192.168.122.200
PING 192.168.122.200 (192.168.122.200) 56(84) bytes of data.
64 bytes from 192.168.122.200: icmp_seq=1 ttl=64 time=1.67 ms
64 bytes from 192.168.122.200: icmp_seq=2 ttl=64 time=0.857 ms
64 bytes from 192.168.122.200: icmp_seq=3 ttl=64 time=0.802 ms
(...)</code></pre></div>

<p>We can ping, but we can&#39;t SSH yet, as that&#39;s still not allowed by default. Let&#39;s try adding another rule, to allow SSH access for all instances in the &#39;default&#39; group:</p>

<div><pre><code class="language-none">$ openstack security group rule create --proto tcp --dst-port 22:22 $SEC_GROUP_ID
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| created_at        | 2018-04-18T14:00:36Z                 |
| description       |                                      |
| direction         | ingress                              |
| ether_type        | IPv4                                 |
| id                | 7e514f09-780c-4a70-97bd-14ecf24fb529 |
| name              | None                                 |
| port_range_max    | 22                                   |
| port_range_min    | 22                                   |
| project_id        | c14b205e428e43319fe43fb0396bd092     |
| protocol          | tcp                                  |
| remote_group_id   | None                                 |
| remote_ip_prefix  | 0.0.0.0/0                            |
| revision_number   | 0                                    |
| security_group_id | 2641d918-579b-4acd-8fbf-894f4e7be241 |
| updated_at        | 2018-04-18T14:00:36Z                 |
+-------------------+--------------------------------------+</code></pre></div>

<p>Let&#39;s quickly verify connectivity.. (The root password is <strong>&#39;redhat&#39;</strong>)</p>

<div><pre><code class="language-none">$ ssh root@192.168.122.200
The authenticity of host &#39;192.168.122.200 (192.168.122.200)&#39; can&#39;t be established.
ECDSA key fingerprint is SHA256:maBYZmeAY6go1ynwTpoZ8o3kjWPgzuePT/6QDXe95rY.
ECDSA key fingerprint is MD5:d3:f4:3d:dc:26:78:44:b2:0a:f1:ee:72:d8:a0:35:d1.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added &#39;192.168.122.200&#39; (ECDSA) to the list of known hosts.
Password:
[root@my-vm ~]#</code></pre></div>

<p>When you see the &quot;<strong>[root@my-vm ~]#</strong>&quot; prompt, you&#39;re connected into your instance successfully. Check the network configuration within the instance, note that it is <em>not</em> aware of the &quot;192.168.122.200&quot; address - this is being NAT&#39;d by the L3 agent from the outside <strong>external</strong> network:</p>

<div><pre><code class="language-none">[root@my-vm ~]# ip address show eth0
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc pfifo_fast state UP qlen 1000
    link/ether fa:16:3e:8e:25:34 brd ff:ff:ff:ff:ff:ff
    inet 172.16.1.7/24 brd 172.16.1.255 scope global dynamic eth0
       valid_lft 84123sec preferred_lft 84123sec
    inet6 fe80::f816:3eff:fe8e:2534/64 scope link
       valid_lft forever preferred_lft forever</code></pre></div>

<p>Press &quot;Ctrl+d&quot;, or simply type &quot;<strong>exit</strong>&quot; to return to your OpenStack environment:</p>

<div><pre><code class="language-none">[root@my-vm ~]# exit
Connection to 192.168.122.200 closed.
$</code></pre></div>

<h1 id="toc_20"><strong>Viewing Console Output (VNC)</strong></h1>

<p>It&#39;s critical to ensure that our virtual machine has started successfully. Nova provides us with two main ways of checking this without relying on networking being available - the console output, and the VNC console output. Using a web-browser, we can view the VNC console via an HTML5 viewer. We can get the URL via the following command:</p>

<div><pre><code class="language-none">$ openstack console url show my_vm --novnc
+-------+--------------------------------------------------------------------------------------+
| Field | Value                                                                                |
+-------+--------------------------------------------------------------------------------------+
| type  | novnc                                                                                |
| url   | http://129.146.150.142:6080/vnc_auto.html?token=1991f8b8-1de0-427e-a9e4-56effdc938aa |
+-------+--------------------------------------------------------------------------------------+</code></pre></div>

<p><strong>WARNING</strong>: You may find that if you run the above command before the machine has booted, you&#39;ll receive the following error. Simply wait a minute or two for the instance to boot.</p>

<div><pre><code class="language-none">$ openstack console url show my_vm --novnc
ERROR: Instance not yet ready (HTTP 409) (Request-ID: req-da372916-10ba-4158-91a2-0cc10f4d083e)</code></pre></div>

<p>You should be able to open this URL directly from your workstation and view the VM&#39;s VNC console; you can also login should you wish, the <strong>root</strong> password is <strong>&#39;redhat&#39;</strong>.</p>

<p><img src="images/vnc-console.png" style="width: 1000px;"/></p>

<h1 id="toc_21"><strong>Connectivity in Browser</strong></h1>

<p>This section is not essential, but you may want to verify that you can access the Horizon dashboard for your environment; let&#39;s open up a web-browser and ensure that it shows all of our resources that we just built up. The URL for this can be found in the email that was sent to you by RHPDS, see the hyperlink in the middle that looks like this - <a href="http://horizon-REPL.rhpds.opentlc.com/dashboard" target="_blank">http://horizon-REPL.rhpds.opentlc.com/dashboard</a> (where REPL is your GUID that was allocated to you when we started), once opened you should see the following:</p>

<p><img src="images/horizon.png" style="width: 1000px;"/></p>

<p>To login, you&#39;ll need to get the automatically generated password from the recently created <strong>~/overcloudrc</strong> file (your password will be different to the output shown below):</p>

<div><pre><code class="language-none">undercloud$ egrep -i &#39;(username|password)&#39; ~/overcloudrc
export OS_USERNAME=admin
export OS_PASSWORD=jZJVX3D4xZaKeDJPfWs8CEBUB
export OS_AUTH_TYPE=password</code></pre></div>

<p>Make sure you select the &#39;<strong>Project</strong>&#39; tab at the top of the screen, as it should take you to the &#39;<strong>Identity</strong>&#39; tab by default as we&#39;re doing everything as the &#39;<strong>admin</strong>&#39; user. Feel free to play around with the OpenStack deployment if you&#39;ve got plenty of time to spare. We can refer back to the web dashboard at a later step where required.</p>

<p><br /></p>

<h2 id="toc_22">Exploring and Validating the OpenDaylight Connectivity</h2>

<p>So far, all we&#39;ve tested is basic OpenStack functionality, albeit configuring some of the basic networking, so in theory OpenDaylight and its connectivity to Neutron is functioning as expected, but we haven&#39;t actually done anything OpenDaylight specific; we would have likely had the exact same functionality with any OpenStack Networking implementation.</p>

<p>Let&#39;s explore the current configuration to verify OpenDaylight integration, and how we can investigate how it all fits together. First let&#39;s ask OpenStack for a list of networking agents and services that are running within our environment:</p>

<div><pre><code class="language-none">$ openstack network agent list
+--------------------------------------+----------------+-------------------------------+-------------------+-------+-------+------------------------------+
| ID                                   | Agent Type     | Host                          | Availability Zone | Alive | State | Binary                       |
+--------------------------------------+----------------+-------------------------------+-------------------+-------+-------+------------------------------+
| 1f9dbb3d-17fc-45dd-9a45-861bdef48c6f | DHCP agent     | summit-networker.localdomain  | nova              | :-)   | UP    | neutron-dhcp-agent           |
| 23e8b46a-bce0-4a94-a8ba-277aa60ed971 | ODL L2         | summit-networker.localdomain  | None              | :-)   | UP    | neutron-odlagent-portbinding |
| 4350636f-26f6-42b0-a942-640939a48972 | Metadata agent | summit-networker.localdomain  | None              | :-)   | UP    | neutron-metadata-agent       |
| b4238dec-1f8d-4b7b-84bd-ed5ffbecee79 | ODL L2         | summit-controller.localdomain | None              | :-)   | UP    | neutron-odlagent-portbinding |
| b4c9fa0b-8c30-46b0-accf-22aff4f2a145 | ODL L2         | summit-compute1.localdomain   | None              | :-)   | UP    | neutron-odlagent-portbinding |
| f0730552-2bcc-45e9-8ac0-6248359b8cca | ODL L2         | summit-compute2.localdomain   | None              | :-)   | UP    | neutron-odlagent-portbinding |
+--------------------------------------+----------------+-------------------------------+-------------------+-------+-------+------------------------------+</code></pre></div>

<p>What you&#39;ll see is that we have three systems that have networking functions, the &#39;<strong>summit-networker</strong>&#39; machine, and the &#39;<strong>summit-computeX</strong>&#39; machines; the <strong>controller</strong> is shown above as it also has ODL enabled by default, which it doesn&#39;t actually need if you&#39;re running dedicated networker nodes like we are.</p>

<p>First thing to note is that the <strong>networker</strong> provides some additional capabilities - DHCP and Metadata. On each of these nodes you&#39;ll also see &#39;<strong>ODL-L2</strong>&#39; running, but how is this possible when earlier we stated that OpenDaylight removes the need for any agents other than DHCP/Metadata, and directly programs Open vSwitch? The answer is these are not real agents; they are in fact called pseudo-agents. The <strong>networking-odl</strong> driver is reading configuration provided by OpenDaylight about each OpenStack node, and then entering it into the Neutron Agent DB as an agent for ML2. This information is used to select the node for ML2 port binding (binding a Neutron port to a physical host). But where does OpenDaylight get this configuration from? This is a good time to take a look at the Open vSwitch configuration on a node:</p>

<div><pre><code class="language-none">$ ssh root@summit-compute1 ovs-vsctl list open_vswitch
_uuid               : 9aa34a4e-efdf-4b9f-98dc-089b71506c97
bridges             : [2180eb91-09d5-44dd-b084-ad042340b15e, ac6193d6-f3b5-4ea5-9a97-9445a2beac7e]
cur_cfg             : 23
datapath_types      : [netdev, system]
db_version          : &quot;7.15.0&quot;
external_ids        : {hostname=&quot;summit-compute1.localdomain&quot;, &quot;odl_os_hostconfig_config_odl_l2&quot;=&quot;{  \&quot;supported_vnic_types\&quot;: [{    \&quot;vnic_type\&quot;: \&quot;normal\&quot;,    \&quot;vif_type\&quot;: \&quot;ovs\&quot;,    \&quot;vif_details\&quot;: {}  }],  \&quot;allowed_network_types\&quot;: [\&quot;local\&quot;,\&quot;vlan\&quot;,\&quot;vxlan\&quot;,\&quot;gre\&quot;],  \&quot;bridge_mappings\&quot;: {\&quot;datacentre\&quot;:\&quot;br-ex\&quot;}}&quot;, odl_os_hostconfig_hostid=&quot;summit-compute1.localdomain&quot;, system-id=&quot;62467b46-8d0c-4124-803f-640e77034668&quot;}
iface_types         : [geneve, gre, internal, lisp, patch, stt, system, tap, vxlan]
manager_options     : [ace09c13-2d72-4372-8394-548b2575f6ed, d1d2e151-b7f1-4900-ab10-61391cf34ed8]
next_cfg            : 23
other_config        : {local_ip=&quot;172.17.2.16&quot;, provider_mappings=&quot;datacentre:br-ex&quot;}
ovs_version         : &quot;2.7.3&quot;
ssl                 : []
statistics          : {}
system_type         : rhel
system_version      : &quot;7.4&quot;</code></pre></div>

<p>Let&#39;s examine the above output.  The <strong>&#39;other_config&#39;</strong> section lists <strong>&#39;local_ip&#39;</strong>, this is source IP that VXLAN based network overlay will use as the source IP for its tunnel. The <strong>&#39;provider_mappings&#39;</strong> works the same way as Neutron bridge mappings and provides the mapping for logical to physical networks on this host.</p>

<p>Additionally look at <strong>&#39;external_ids&#39;</strong>.  The <strong>&#39;odl_os_hostconfig_config_odl_l2&#39;</strong> section contains more information (some duplicate for legacy reasons) which indicates what kind of ports this node supports (vhostuser DPDK, or normal OVS VIF ports), the allowed network types on this node, etc. This information is read by OpenDaylight using the OVSDB protocol and eventually propagated by networking-odl into the Neutron Agent DB.
Note, in Red Hat OpenStack Platform 12 when a port binding event occurs, networking-odl automatically sets the port to Active state.  Typically in port binding a port should get created in Neutron then a ML2 port binding will execute a bind call to the driver (in this case ODL) who will verify the port is bound on the correct host. In Red Hat OpenStack Platform 13, there is support for a new web-socket based connection which runs over port 8185 between OpenDaylight and Neutron.  This allows OpenDaylight to update the port state to <strong>Active</strong> once it sees the port created on the compute node and bound correctly to the virtual Neutron network.</p>

<p>The <strong>ODL-L2</strong> agent configures the connections for OVSDB and OpenFlow, and are configured within Open vSwitch; each is described below:</p>

<table>
<thead>
<tr>
<th>Connection Type</th>
<th>Host/Port Number</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>ODL Southbound OVSDB</td>
<td>odl-controller:6640</td>
<td>OVSDB is used to manage switch configuration.  While OpenFlow (see below) allows us to configure the <strong>datapath</strong> table of a switch, we also need a protocol to allow configuration of ports, bridges and other settings on the switch.  OVSDB gives us that ability to do switch configuration management, and this connection is used by OpenDaylight to program the local Open vSwitch.</td>
</tr>
<tr>
<td>ODL Southbound OpenFlow</td>
<td>odl-controller:6653</td>
<td>OpenFlow is a protocol that was designed with SDN in mind.  In the network data path it is represented as a list of rules that determine how packets are forwarded.  The concept of a rule is broken down into <strong>&#39;match&#39;</strong> criteria (to match on a packet) and then an <strong>&#39;action&#39;</strong> (what to do with the packet). The control plane side of OpenFlow allows a centralised OpenFlow controller (such as ODL) to push these rules into remote switches and control datapath forwarding throughout a network from a centralised controller. This connection is how OpenDaylight configures the flows of the local Open vSwitch.</td>
</tr>
<tr>
<td>Local OVSDB Connection</td>
<td>localhost:6639</td>
<td>This is configured by Director as the local port to run OVSDB Server on. By default OVSDB Server runs on port 6640, but since that port is taken by ODL on the control nodes, we reconfigure the local OVSDB server to use 6639. This port is what Neutron&#39;s <strong>DHCP agent</strong> uses to configure OVS with DHCP ports. Note that all nodes are configured with this, regardless of whether the DHCP agent is started.</td>
</tr>
</tbody>
</table>

<p>If we look at the Open vSwitch configuration on our <strong>summit-networker</strong> machine we can see how all of this has been configured. To do this, we&#39;ll need to briefly ssh to that machine and output the entire configuration; this is annotated below:</p>

<div><pre><code class="language-none">$ ssh root@summit-networker ovs-vsctl show
60ccb669-1eb9-4097-b915-8f02ac6f46e3
Manager &quot;ptcp:6639:127.0.0.1&quot;                      &lt;--- Local OVSDB Server listener (DHCP)
Manager &quot;tcp:172.17.1.16:6640&quot;                     &lt;--- ODL Southbound OVSDB
    is_connected: true
Bridge br-ex                                       &lt;--- the br-ex OVS bridge
    fail_mode: standalone
    Port br-ex-int-patch
        Interface br-ex-int-patch
            type: patch
            options: {peer=br-ex-patch}
    Port &quot;vlan101&quot;                                 &lt;--- additional VLAN interfaces
        tag: 101                                        for other OpenStack traffic
        Interface &quot;vlan101&quot;                             e.g. storage/internal API networks
            type: internal
    Port &quot;eth1&quot;                                    &lt;--- physical eth1 nic
        Interface &quot;eth1&quot;
    Port &quot;vlan201&quot;
        tag: 201
        Interface &quot;vlan201&quot;
            type: internal
    Port br-ex
        Interface br-ex
            type: internal
Bridge br-int                                      &lt;--- integration bridge
    Controller &quot;tcp:172.17.1.16:6653&quot;              &lt;--- ODL Southbound OpenFlow
        is_connected: true
    fail_mode: secure
    Port br-int
        Interface br-int
            type: internal
    Port &quot;tape42503b6-de&quot;                          &lt;--- DHCP agent TAP device
        Interface &quot;tape42503b6-de&quot;
            type: internal
    Port &quot;tun86ef5582407&quot;                          &lt;--- VXLAN tunnel endpoint
        Interface &quot;tun86ef5582407&quot;                      to a compute node
            type: vxlan
            options: {key=flow, local_ip=&quot;172.17.2.20&quot;, remote_ip=&quot;172.17.2.19&quot;} &lt;--- local_ip from OVS configuration
    Port br-ex-patch
        Interface br-ex-patch
            type: patch
            options: {peer=br-ex-int-patch}
ovs_version: &quot;2.7.3&quot;</code></pre></div>

<blockquote>
<p><strong>NOTE:</strong> We don&#39;t need to specify a password to execute remote commands on the networker node - we&#39;ve already pre-populated the overcloud nodes with a secure shell key for convenience, but you may have to accept the key.</p>

<p><strong>NOTE</strong>: Tunnel devices are created when required rather than establishing a full mesh network, hence why the output above only shows a single connection to one of our compute nodes, and not the other one. Recall that we only have <strong>one</strong> instance running, and it would have been scheduled upon one of those nodes, not both.</p>
</blockquote>

<p>In addition, you&#39;ll also note that bridge &quot;br-ex&quot; is already attached to a real world physical network interface, <strong>&quot;eth1&quot;</strong>. The important thing to understand here is that Neutron only understands <strong>logical</strong> &quot;physical network&quot; names - these logical network names are translated into real underlying networks via a network bridge by the plugin that we&#39;re using (in our case, OpenDaylight). To understand how this works, we need to look at the defined logical networks that OVS/OpenDaylight is exposing to Neutron via the &#39;<strong>bridge_mappings</strong>&#39; extension. Select the &#39;<strong>id</strong>&#39; for the ODL-L2 pseudo agent on &#39;<strong>summit-networker</strong>&#39; in the following command, and it will show you the current bridge mappings:</p>

<div><pre><code class="language-none">$ openstack network agent show 23e8b46a-bce0-4a94-a8ba-277aa60ed971 -f json | grep -A2 bridge_mappings
    &quot;bridge_mappings&quot;: {
        &quot;datacentre&quot;: &quot;br-ex&quot;
    }</code></pre></div>

<p>What this shows it that we have a physical network name of &quot;<strong>datacentre</strong>&quot;, which if used, would tell Neutron to route all external traffic for that network onto the Open vSwitch bridge &quot;<strong>br-ex</strong>&quot;. Recall that when we created our external network, we defined it as using the <strong>&#39;datacentre&#39;</strong> logical network, hence the mapping here. To re-iterate, when we defined our external network with the Neutron logical name <strong>&quot;datacentre&quot;</strong>, the traffic utilises <strong>&quot;eth1&quot;</strong> as a physical network via the Open vSwitch bridge <strong>&quot;br-ex&quot;</strong>. </p>

<p>Let&#39;s now explore how Neutron is communicating with OpenDaylight, showing that through the ML2 interface Neutron networks are represented within OpenDaylight and are then implemented within the Open vSwitch configuration on the relevant nodes. For this, we need to query the OpenDaylight REST API and ask it for a list of networks. By default, the OpenDaylight controller as deployed by TripleO is not accessible from the external or public network, but is available on both the control plane network and the internal API network within the overcloud. These endpoints, like many other OpenStack services, are maintained by the HA Proxy configuration on the overcloud controller.</p>

<p>Let&#39;s first get the control plane virtual IP address so we know how to contact our OpenDaylight controller, for this we can request a list from the undercloud, making sure that you&#39;ve sourced your <strong>stackrc</strong> (undercloud) environment file, <strong>not</strong> the overcloud one:</p>

<div><pre><code class="language-none">undercloud$ source ~/stackrc
undercloud$ openstack stack output show overcloud VipMap
+--------------+------------------------------------------------------------------------+
| Field        | Value                                                                  |
+--------------+------------------------------------------------------------------------+
| description  | Mapping of each network to VIP addresses. Also includes the Redis VIP. |
| output_key   | VipMap                                                                 |
| output_value | {                                                                      |
|              |   &quot;storage&quot;: &quot;172.17.3.13&quot;,                                            |
|              |   &quot;management_uri&quot;: &quot;&quot;,                                                |
|              |   &quot;internal_api_subnet&quot;: &quot;&quot;,                                           |
|              |   &quot;ctlplane&quot;: &quot;172.16.0.23&quot;,                                           |
|              |   &quot;storage_uri&quot;: &quot;172.17.3.13&quot;,                                        |
|              |   &quot;management&quot;: &quot;&quot;,                                                    |
|              |   &quot;management_subnet&quot;: &quot;&quot;,                                             |
|              |   &quot;redis&quot;: &quot;172.17.1.10&quot;,                                              |
|              |   &quot;storage_subnet&quot;: &quot;&quot;,                                                |
|              |   &quot;storage_mgmt_uri&quot;: &quot;172.17.4.19&quot;,                                   |
|              |   &quot;tenant_uri&quot;: &quot;&quot;,                                                    |
|              |   &quot;external&quot;: &quot;192.168.122.100&quot;,                                       |
|              |   &quot;storage_mgmt&quot;: &quot;172.17.4.19&quot;,                                       |
|              |   &quot;tenant&quot;: &quot;&quot;,                                                        |
|              |   &quot;tenant_subnet&quot;: &quot;&quot;,                                                 |
|              |   &quot;ctlplane_uri&quot;: &quot;172.16.0.23&quot;,                                       |
|              |   &quot;external_subnet&quot;: &quot;&quot;,                                               |
|              |   &quot;storage_mgmt_subnet&quot;: &quot;&quot;,                                           |
|              |   &quot;internal_api&quot;: &quot;172.17.1.15&quot;,                                       |
|              |   &quot;internal_api_uri&quot;: &quot;172.17.1.15&quot;,                                   |
|              |   &quot;external_uri&quot;: &quot;192.168.122.100&quot;,                                   |
|              |   &quot;ctlplane_subnet&quot;: &quot;172.16.0.23/24&quot;                                  |
|              | }                                                                      |
+--------------+------------------------------------------------------------------------+</code></pre></div>

<p>In my environment, it&#39;s been allocated the IP address <strong>172.16.0.23</strong> (<strong>ctlplane</strong>), but your environment will very likely be different. We could have fixed this during the deployment, but we&#39;ve opted to minimise the complexity of the templates. Let&#39;s export this as an environment variable, making sure that you use the IP address that was selected for your environment.</p>

<div><pre><code class="language-none">undercloud$ export CTLPLANE=172.16.0.23</code></pre></div>

<p>Next, let&#39;s call the REST API for OpenDaylight and ask it for a list of Neutron networks, noting that the default username and password is &#39;<strong>admin/admin</strong>&#39;:</p>

<div><pre><code class="language-none">undercloud$ curl -u admin:admin http://$CTLPLANE:8081/controller/nb/v2/neutron/networks
{
   &quot;networks&quot; : [ {
      &quot;id&quot; : &quot;642f4496-7773-4d17-8f82-52fe8efc2a62&quot;,
      &quot;tenant_id&quot; : &quot;c14b205e428e43319fe43fb0396bd092&quot;,
      &quot;project_id&quot; : &quot;c14b205e428e43319fe43fb0396bd092&quot;,
      &quot;revision_number&quot; : 3,
      &quot;name&quot; : &quot;external&quot;,                                        &lt;-- our &#39;external&#39; network
      &quot;admin_state_up&quot; : true,
      &quot;status&quot; : &quot;ACTIVE&quot;,
      &quot;shared&quot; : false,
      &quot;router:external&quot; : true,
      &quot;provider:network_type&quot; : &quot;flat&quot;,
      &quot;provider:physical_network&quot; : &quot;datacentre&quot;,
      &quot;segments&quot; : [ ]
   }, {
      &quot;id&quot; : &quot;bd8db3a8-2b30-4083-a8b3-b3fd46401142&quot;,
      &quot;tenant_id&quot; : &quot;bd8db3a82b304083a8b3b3fd46401142&quot;,
      &quot;project_id&quot; : &quot;bd8db3a8-2b30-4083-a8b3-b3fd46401142&quot;,
      &quot;name&quot; : &quot;Sync Canary Network&quot;,
      &quot;admin_state_up&quot; : false,
      &quot;status&quot; : &quot;ACTIVE&quot;,
      &quot;shared&quot; : false,
      &quot;router:external&quot; : false,
      &quot;provider:network_type&quot; : &quot;flat&quot;,
      &quot;segments&quot; : [ ]
   }, {
      &quot;id&quot; : &quot;22f9a9cd-1da0-4c6a-a815-d25da07aa18d&quot;,
      &quot;tenant_id&quot; : &quot;c14b205e428e43319fe43fb0396bd092&quot;,
      &quot;project_id&quot; : &quot;c14b205e428e43319fe43fb0396bd092&quot;,
      &quot;revision_number&quot; : 2,
      &quot;name&quot; : &quot;internal&quot;,                                       &lt;-- our &#39;internal&#39; network
      &quot;admin_state_up&quot; : true,
      &quot;status&quot; : &quot;ACTIVE&quot;,
      &quot;shared&quot; : false,
      &quot;router:external&quot; : false,
      &quot;provider:network_type&quot; : &quot;vxlan&quot;,
      &quot;provider:segmentation_id&quot; : &quot;30&quot;,
      &quot;segments&quot; : [ ]
   } ]
}</code></pre></div>

<p>You should see three networks listed, the VXLAN-based internal tenant network called &quot;<strong>internal</strong>&quot; and the external network called &quot;<strong>external</strong>&quot;, both of which we created earlier, that&#39;s used for floating IP access and external routing (noting that it&#39;s associated to the flat &quot;datacentre&quot; physical network), and you&#39;ll note that the ID&#39;s also match up (remember to source the ~/overcloudrc file again):</p>

<div><pre><code class="language-none">$ source ~/overcloudrc
$ openstack network list -c ID -c Name
+--------------------------------------+----------+
| ID                                   | Name     |
+--------------------------------------+----------+
| 22f9a9cd-1da0-4c6a-a815-d25da07aa18d | internal |
| 642f4496-7773-4d17-8f82-52fe8efc2a62 | external |
+--------------------------------------+----------+</code></pre></div>

<p>And then a third network called the &quot;<strong>Sync Canary Network</strong>&quot;. The canary network is a dummy network used as a placeholder, created by the ML2 OpenDaylight service as a mechanism to check whether we&#39;re in a consistent state between OpenDaylight and Neutron. More specifically, if this network has been removed, it&#39;s assumed that the OpenDaylight network database has been dropped, and it will trigger a full re-sync between Neutron and OpenDaylight to ensure consistency. The fact that these networks are known by OpenDaylight, aside from the fact that we&#39;ve validated that basic networking is working, prove that Neutron is able to successfully communicate with the OpenDayight SDN controller.</p>

<h3 id="toc_23">OpenDaylight Under the Covers</h3>

<p>In this section we&#39;re going to take a look at the OpenDaylight controller itself; where it runs, how you can connect into it, query it for information, and what functions are available. In our model the OpenDaylight controller runs on our dedicated networker machine, so let&#39;s hop over to it:</p>

<div><pre><code class="language-none">$ ssh root@summit-networker</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: If the following commands are prefixed with &quot;<strong>networker#</strong>&quot; it signifies that the commands are to be executed on the summit-networker machine and not the undercloud.</p>
</blockquote>

<p>Now we can connect to the OpenDaylight management command-line console, which runs over secure shell. This listens on port <strong>8101</strong> on the nodes running an instance of the OpenDaylight controller:</p>

<div><pre><code class="language-none">networker# netstat -tunpl | grep 8101
tcp6       0      0 :::8101                 :::*           LISTEN      2190/java</code></pre></div>

<p>But it&#39;s actually running in a Docker container (in OSP12+ the vast majority of OpenStack services are now containerised as opposed to installed as standard RPM&#39;s and managed through systemd) but we&#39;ll explore this a bit further later on:</p>

<div><pre><code class="language-none">networker# docker ps -a | grep opendaylight
60e5bba8875e    172.16.0.1:8787/rhosp12/openstack-opendaylight:12.0-20180319.1   &quot;kolla_start&quot;       3 weeks ago         Up 10 hours            opendaylight_api</code></pre></div>

<p>Let&#39;s connect into this console from the networker node itself (password is &#39;<strong>karaf</strong>&#39;):</p>

<div><pre><code class="language-none">networker# ssh karaf@localhost -p 8101
Password authentication
Password:

    ________                       ________                .__  .__       .__     __
    \_____  \ ______   ____   ____ \______ \ _____  ___.__.|  | |__| ____ |  |___/  |_
     /   |   \\____ \_/ __ \ /    \ |    |  \\__  \&lt;   |  ||  | |  |/ ___\|  |  \   __\
    /    |    \  |_&gt; &gt;  ___/|   |  \|    `   \/ __ \\___  ||  |_|  / /_/  &gt;   Y  \  |
    \_______  /   __/ \___  &gt;___|  /_______  (____  / ____||____/__\___  /|___|  /__|
            \/|__|        \/     \/        \/     \/\/            /_____/      \/


Hit &#39;&lt;tab&gt;&#39; for a list of available commands
and &#39;[cmd] --help&#39; for help on a specific command.
Hit &#39;&lt;ctrl-d&gt;&#39; or type &#39;system:shutdown&#39; or &#39;logout&#39; to shutdown OpenDaylight.

opendaylight-user@root&gt;</code></pre></div>

<p>Here, we can run some additional commands to inspect the current networking status. Let&#39;s start with seeing whether we can match the Open vSwitch configuration setup for our VXLAN-based tunnel network endpoints to the structure that OpenDaylight knows about:</p>

<div><pre><code class="language-none">opendaylight$ vxlan:show
Name                                              Description
Local IP                 Remote IP                Gateway IP         AdmState
OpState                  Parent                   Tag
--------------------------------------------------------------------------------
tund35bcb4915e                                    VXLAN Trunk Interface
172.17.2.19              172.17.2.20              0.0.0.0            ENABLED
UP                       215658939848679/tund35bcb4915e 7

tun86ef5582407                                    VXLAN Trunk Interface
172.17.2.20              172.17.2.19              0.0.0.0            ENABLED
UP                       97146301947967/tun86ef5582407 8</code></pre></div>

<p>As you can see, this matches the output shown in Open vSwitch, remembering that the only nodes that have networking connectivity at the moment are the networker to the first compute node that&#39;s running our instance. If we were to launch an additional instance, it&#39;s likely that this would be extended to accommodate connectivity to the second compute node.</p>

<p>Next, another basic command we can use is to list the NAPT switches/routers that are used to perform SNAT functionality for instances that do not have floating IP&#39;s, noting that the router ID will match the Neutron router ID that we created earlier in the lab:</p>

<div><pre><code class="language-none">opendaylight$ odl:display-napt-switches
 Router Id                             Datapath Node Id      Managment Ip Address
-------------------------------------------------------------------------------------------
 fa88e28a-34c4-4b8e-8c33-b67e54c4c9c0  233871321246373       172.17.2.16</code></pre></div>

<p>The IP address of the NAPT switch, <strong>172.17.2.16</strong>, is the IP address of a compute node where OpenDaylight has deployed a virtual router to perform the SNAT functionality, allowing us to have distributed routing and is using <strong>conntrack</strong> to manage it. Note that it&#39;s not currently possible to have highly-available SNAT routers with the OpenDaylight integration.</p>

<p>Let&#39;s quit out of our OpenDaylight controller and return to our undercloud machine for now:</p>

<div><pre><code class="language-none">opendaylight$ (Ctrl-D, or &#39;logout&#39;)
Connection to localhost closed.
networker# exit
Connection to 172.16.0.25 closed.</code></pre></div>

<p>Many of the OpenStack-specific implementations don&#39;t have an equivalent Karaf console command that we can use, and therefore we sometimes have to use the REST API like we did to query the networks earlier. Let&#39;s check the floating IP mappings that are being used as an example:</p>

<div><pre><code class="language-none">undercloud$ $ curl -s -u admin:admin \
    http://$CTLPLANE:8081/restconf/config/odl-nat:floating-ip-info | \
    python -m json.tool

{
    &quot;floating-ip-info&quot;: {
        &quot;router-ports&quot;: [
            {
                &quot;external-network-id&quot;: &quot;642f4496-7773-4d17-8f82-52fe8efc2a62&quot;,
                &quot;ports&quot;: [
                    {
                        &quot;internal-to-external-port-map&quot;: [
                            {
                                &quot;external-id&quot;: &quot;4965c561-c97e-48c0-b89e-7f9064d399f6&quot;,
                                &quot;external-ip&quot;: &quot;192.168.122.200&quot;,
                                &quot;internal-ip&quot;: &quot;172.16.1.7&quot;
                            }
                        ],
                        &quot;port-name&quot;: &quot;bcc0f8b1-9450-4e7d-b19f-0d0d0e1942f4&quot;
                    }
                ],
                &quot;router-id&quot;: &quot;fa88e28a-34c4-4b8e-8c33-b67e54c4c9c0&quot;
            }
        ]
    }
}</code></pre></div>

<p>Here we have our floating IP matching our instance IP, as demonstrated through the Neutron API too:</p>

<div><pre><code class="language-none">$ openstack floating ip show 192.168.122.200
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| created_at          | 2018-04-18T13:35:49Z                 |
| description         |                                      |
| fixed_ip_address    | 172.16.1.7                           |       &lt;--- &quot;internal-ip&quot;
| floating_ip_address | 192.168.122.200                      |       &lt;--- &quot;external-ip&quot;
| floating_network_id | 642f4496-7773-4d17-8f82-52fe8efc2a62 |
| id                  | 4965c561-c97e-48c0-b89e-7f9064d399f6 |
| name                | 192.168.122.200                      |
| port_id             | bcc0f8b1-9450-4e7d-b19f-0d0d0e1942f4 |
| project_id          | c14b205e428e43319fe43fb0396bd092     |
| revision_number     | 1                                    |
| router_id           | fa88e28a-34c4-4b8e-8c33-b67e54c4c9c0 |
| status              | ACTIVE                               |
| updated_at          | 2018-04-18T13:36:33Z                 |
+---------------------+--------------------------------------+</code></pre></div>

<p>As you can see, there are a number of different ways of interacting with OpenDaylight; Neutron can provide a direct route to a lot of the data, but this gets translated into OpenDaylight specific constructs by the ML2 OpenDaylight implementation. It&#39;s then the responsibility of OpenDaylight to program the local Open vSwitch implementations at each node that requires it.</p>

<h1 id="toc_24">Connecting to the OpenDaylight Container</h1>

<p>Red Hat OpenStack Platform, as per the default configuration in OSP12 and beyond, utilises containers for the vast majority of the OpenStack services. OpenDaylight is no exception here, and as such, on our dedicated networker node the OpenDaylight SDN Controller runs within a docker container. If you ever need to access the container where OpenDaylight is running, e.g. if it&#39;s not running correctly and the API access is not sufficient, then there are a number of different ways you can interact with it.</p>

<p>Firstly, you can query the logs that the container is producing. For this you need to make sure that you&#39;re connected to the host that&#39;s running our OpenDaylight controller:</p>

<div><pre><code class="language-none">undercloud$ ssh root@summit-networker
networker# # docker ps -a
CONTAINER ID        IMAGE                                                            COMMAND             CREATED             STATUS                    PORTS               NAMES
34e2e1a71f09        172.16.0.1:8787/rhosp12/openstack-cron:12.0-20171129.1           &quot;kolla_start&quot;       15 hours ago        Up 15 hours                                   logrotate_crond
3e94f63b3068        172.16.0.1:8787/rhosp12/openstack-opendaylight:12.0-20171129.1   &quot;kolla_start&quot;       15 hours ago        Up 15 hours                      opendaylight_api</code></pre></div>

<p>Here you&#39;ll see that this host is only running two containers - recall that this is actually a very simple role that just runs OpenDaylight in a container; with OSP12, all other OpenStack networking services are not yet containerised, primarily to allow third party vendors that provide networking solutions can have some time to move over to the docker packaging format. To verify, you can demonstrate that Neutron&#39;s DHCP service is still managed by systemd:</p>

<div><pre><code class="language-none">networker# systemctl status neutron-dhcp-agent.service
● neutron-dhcp-agent.service - OpenStack Neutron DHCP Agent
   Loaded: loaded (/usr/lib/systemd/system/neutron-dhcp-agent.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2018-04-18 12:21:15 UTC; 10h ago
 Main PID: 24284 (neutron-dhcp-ag)
   Memory: 102.9M
   CGroup: /system.slice/neutron-dhcp-agent.service
           ├─24284 /usr/bin/python2 /usr/bin/neutron-dhcp-agent --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/dhcp_agent...
           ├─92414 sudo neutron-rootwrap-daemon /etc/neutron/rootwrap.conf
           ├─92415 /usr/bin/python2 /usr/bin/neutron-rootwrap-daemon /etc/neutron/rootwrap.conf
           ├─92486 dnsmasq --no-hosts --no-resolv --strict-order --except-interface=lo --pid-file=/var/lib/neutron/dhcp/1e357bee-87e5-4074-b87a-a418778b99e8/pid --dhcp-hostsfile=/var/lib/neutr...
           └─92488 haproxy -f /var/lib/neutron/ns-metadata-proxy/1e357bee-87e5-4074-b87a-a418778b99e8.conf</code></pre></div>

<p>Going back to the OpenDaylight container, to view the logs you simply need to use the &#39;docker logs&#39; command, with the ID of the container provided from the list we got earlier:</p>

<div><pre><code class="language-none">networker# docker logs 3e94f63b3068
(...)</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: The above output is removed simply due to its verbosity. But this can be used to track down startup issues, or any issues that may be logged, helping you troubleshoot any problems.</p>
</blockquote>

<p>Let&#39;s say that you get this far and you want to go a little deeper; it&#39;s possible to attach into a container to execute commands from within the container, interacting with all namespaces associated with that container:</p>

<div><pre><code class="language-none">networker# docker exec -it 3e94f63b3068 /bin/bash
tput: No value for $TERM and no -T specified
tput: No value for $TERM and no -T specified
container# export TERM=xterm</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: We export $TERM to help the console output properly, as without this you may find that viewing configuration files or log files is very difficult.</p>
</blockquote>

<p>Now you can view the processes, for example, for that namespace, and have direct access to log files, configuration files, and anything that the container requires, as if it was running on the baremetal host:</p>

<div><pre><code class="language-none">container# ps -ax
  PID TTY      STAT   TIME COMMAND
  1 ?           Ssl    77:33 /usr/bin/java -Djava.security.properties=/opt/opendaylight/etc/odl.java.security -server -Xms128M -Xmx2048m -XX:+UnlockDiagnosticVMOptions -XX:+UnsyncloadClass -XX:+H
  45476 ?        Ss     0:00 /bin/bash
  45530 ?        R+     0:00 ps -ax</code></pre></div>

<p>It&#39;s just as easy to disconnect from the container when required:</p>

<div><pre><code class="language-none">container# exit
networker# exit
Connection to 172.16.0.25 closed.
undercloud$</code></pre></div>

<h2 id="toc_25"><strong>Exploration of TripleO Requirements for OpenDaylight</strong></h2>

<p>In this section we&#39;re going to be looking at how we can utilise OSP director (through TripleO) to deploy an OpenStack overcloud with integrated OpenDaylight, and we&#39;ll look at the customisations available with the TripleO heat templates. We already have a pre-deployed environment available to us, but we&#39;ll remove this and go through all of the steps to configure and kick-off a new deployment. Unfortunately it&#39;s not likely that you&#39;ll see the deployment successfully complete given the time constraints of this lab, but at a very minimum you should be able to understand the flows and how the deployment is started.</p>

<p>Let&#39;s first remove/clean-up the existing deployment and reset our environment, remembering that we now have to use the undercloud and as such must source the <strong>~/stackrc</strong> file:</p>

<div><pre><code class="language-none">$ source ~/stackrc</code></pre></div>

<p>Now we can ask the undercloud to remove the existing overcloud stack:</p>

<div><pre><code class="language-none">$ openstack stack delete overcloud
Are you sure you want to delete this stack(s) [y/N]? yes</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: This process will take a few minutes so we&#39;ll let it process the removal in the background and move on.</p>
</blockquote>

<p><br />
Support for the deployment and integration of OpenDaylight has been available within TripleO for a number of releases, and is fully composable like all other OpenStack services, i.e. it can be deployed within any existing role, or a brand new role (e.g. a dedicated networker). Since OSP12, OpenStack services (with the exception of a few core components) are containerised, and are deployed as immutable containers across a set of hosts; OpenDaylight is no exception here, although TripleO currently supports the option of deploying it as a legacy service (&quot;<strong>puppet</strong>&quot; based) or as a container (&quot;<strong>docker</strong>&quot; based).</p>

<p>We&#39;re going to be using the containerised OpenDaylight service in our new deployment, which means that we do not need to install any software in the default RHEL image that OpenStack Ironic rolls out during the node deployment phase. The next few steps are going to walk you through the following tasks:</p>

<ul>
<li>Creating a custom (composable) role for the networking service, one that will take responsibility for running the OpenDaylight services, just like the dedicated networker node we had in the pre-deployed environment.<br /><br /></li>
<li>Configuring the bare metal node configuration to enable role tagging - so each node gets deployed with the correct role as per the hardware specification it requires, including the dedicated networker.<br /><br /></li>
<li>Investigating the &quot;physical&quot; networking topology and configuration for each role, and how OSP director configures the networking on our nodes accordingly.<br /><br /></li>
<li>Understanding the options for OpenDaylight configuration through the TripleO Heat templates, and how this maps to the physical networking topology that we&#39;ve applied.</li>
</ul>

<p>Much of the OpenDaylight requirements are very similar to a standard OpenStack networking experience, and therefore the implementations and configuration options do not differ too much between the two.</p>

<h3 id="toc_26">Creating a Dedicated Networker Role</h3>

<p>Since OSP10 (based on OpenStack Newton), OSP director supports <strong>composable roles</strong> - this allows organisations to break down the legacy OpenStack roles, such as a controller, into individual services and build up custom roles to suit their requirements. This could be as simple as combining a compute and a storage role to create a hyperconverged configuration, or perhaps fragmenting the monolithic controller role to split out some of the services to enable a greater degree of scalability. Historically, all networking services were provided by the controllers; now we have a choice about where the networking services should operate. As part of this section we&#39;re going to demonstrate how we can utilise composable roles to choose how, and where, to deploy the OpenDaylight services.</p>

<p>We&#39;re going to be creating a dedicated networker role, in which all OpenStack networking related services, including the OpenDaylight controller, will reside. For us to understand how this works we must first explore the TripleO heat templates that dictate and control the role structure, and then we&#39;ll look to make some minor modifications to suit our desired configuration.</p>

<p>The templates that are fed into Heat are dynamically generated at deployment time through Jinja2 templates based on the roles that are defined by the administrator. If no custom role information is supplied it will use the default configuration residing at <strong><em>/usr/share/openstack-tripleo-heat-templates/roles_data.yaml</em></strong>, which lists all of the individual OpenStack services (and supporting functions) that it needs to deploy for each of the listed roles. By either modifying this file (not recommended) or supplying a customised role information file at deployment time, one can dynamically define new roles and specify the services in which it will be associated with.</p>

<p>Inside of this file it&#39;s possible to both create new roles dynamically, or simply move services between the existing role structures that are already well understood. In our environment we&#39;re going to be pulling the networking functions (including OpenDaylight) out of the controller and placing them into a new role called &quot;<strong>Networker</strong>&quot;. For this, we can use TripleO to generate some of the role definitions for us. Let&#39;s first make a directory that we&#39;ll use to hold all of our customised templates:</p>

<div><pre><code class="language-none">$ mkdir -p ~/templates/
$ cd ~/templates/</code></pre></div>

<p>Next, let&#39;s generate a new custom roles file, specifying the three main roles that we want to cover, i.e. a &quot;Controller&quot;, a &quot;Compute&quot;, and a &quot;Networker&quot;; noting that TripleO already has the construct of a dedicated networker node available, so it knows which services to allocated into this role:</p>

<div><pre><code class="language-none">$ openstack overcloud roles generate \
    Controller Compute Networker -o ~/templates/lab_roles.yaml</code></pre></div>

<p>So how does this differ from the standard configuration? Well, firstly, the TripleO concept of a networker node is very slightly different from what we&#39;re trying to achieve with our deployment. Namely, the default TripleO networker role still assumes that we want the OpenDaylight SDN controller itself to reside within the <strong>controller</strong> role, but for the purposes of this exercise we&#39;re going to move this to our dedicated <strong>networker</strong> role, along with all Neutron based services except the Neutron API server, which is best left on the controller.</p>

<p>For this, we need to move the service called <strong>&quot;OS::TripleO::Services::OpenDaylightApi&quot;</strong> from the controller, along with all Neutron agents to the networker role, so OSP director knows that it needs to put it them on the networker and not the controller nodes. Let&#39;s make this change, first by removing the instance of <strong>&#39;OpenDaylightApi&#39;</strong> from the controller role, noting that as there&#39;s only one we can use a simple sed command:</p>

<div><pre><code class="language-none">$ sed -i /OpenDaylightApi/d ~/templates/lab_roles.yaml</code></pre></div>

<p>Now we can add that service to the end of the file as the Networker role is at the bottom of the custom roles file and it&#39;s a list of services, noting that whitespace is very important here to ensure the hierarchy of the yaml formatted file:</p>

<div><pre><code class="language-none">undercloud$ echo &quot;    - OS::TripleO::Services::OpenDaylightApi&quot; \
    &gt;&gt; ~/templates/lab_roles.yaml
undercloud$ echo &quot;    - OS::TripleO::Services::NeutronCorePlugin&quot; \
    &gt;&gt; ~/templates/lab_roles.yaml</code></pre></div>

<p>Now, we need to remove all Neutron services/agents/daemons (with the exception of Neutron API and the Neutron Core Plugin service, something used to configure Neutron&#39;s API server to talk to the chosen plugin) from the controller. TripleO, via the roles file generation tool, has already ensured that the networker node has these roles already satisfied.</p>

<p>Next, edit the file, and remove all lines containing &quot;<strong>OS::TripleO::Services:Neutron</strong>&quot; from the <strong>Controller</strong> block.</p>

<p>You can use your favourite text editor for this...</p>

<div><pre><code class="language-none">$ vi ~/templates/lab_roles.yaml</code></pre></div>

<p>You&#39;ll need to <strong>remove</strong> these services (by simply deleting the line in the file):</p>

<ul>
<li><strong>OS::TripleO::Services::NeutronBgpVpnApi</strong></li>
<li><strong>OS::TripleO::Services::NeutronDhcpAgent</strong></li>
<li><strong>OS::TripleO::Services::NeutronL2gwAgent</strong></li>
<li><strong>OS::TripleO::Services::NeutronL2gwApi</strong></li>
<li><strong>OS::TripleO::Services::NeutronL3Agent</strong></li>
<li><strong>OS::TripleO::Services::NeutronLbaasv2Agent</strong></li>
<li><strong>OS::TripleO::Services::NeutronLinuxbridgeAgent</strong></li>
<li><strong>OS::TripleO::Services::NeutronMetadataAgent</strong></li>
<li><strong>OS::TripleO::Services::NeutronML2FujitsuCfab</strong></li>
<li><strong>OS::TripleO::Services::NeutronML2FujitsuFossw</strong></li>
<li><strong>OS::TripleO::Services::NeutronOvsAgent</strong></li>
<li><strong>OS::TripleO::Services::NeutronVppAgent</strong></li>
</ul>

<p>But <strong>don&#39;t</strong> remove the following services:</p>

<ul>
<li><strong>OS::TripleO::Services::NeutronApi</strong></li>
<li><strong>OS::TripleO::Services::NeutronCorePlugin</strong></li>
</ul>

<p>If you&#39;re not comfortable modifying these files yourself, and just want to proceed past this, you can copy the pre-built file that we&#39;ve made available for you, note that you should <strong>only</strong> execute the following command if you just want the easy ride here:</p>

<div><pre><code class="language-none">$ cp ~/labs/odl/templates/custom_roles.yaml ~/templates/lab_roles.yaml</code></pre></div>

<p>What we&#39;re saying here is that the following services have been moved from the controller role to our new networker role:</p>

<ul>
<li>All of Neutron&#39;s standard agents (DHCP, L3, Metadata, LBaaS, OVS etc.)</li>
<li>OpenDaylight&#39;s API Service</li>
</ul>

<p>To integrate OpenDaylight, the TripleO engineers had to add two TripleO services to the list of available :</p>

<ul>
<li>The <strong>OpenDaylightApi</strong> service, which is the TripleO service that is responsible for deploying and operating the OpenDaylight SDN controller itself, this is what we&#39;ve pulled from the default controller role and placed into the custom <strong>networker</strong> role. OpenDaylight offers High Availability (HA) by scaling the number of <strong>OpenDaylightApi</strong> service instances by at least three.  This means that by default, scaling the number of networker nodes to three or more will automatically enable HA.<br /><br /></li>
<li>The <strong>OpenDaylightOvs</strong> service, which is responsible for configuring Open vSwitch on each of the compute nodes and the networker node itself to properly communicate with OpenDaylight, i.e. take its instructions from OpenDaylight. Note that as networker node will be providing additional Neutron capabilities such as DHCP, the local Open vSwitch configuration also needs to be configured by OpenDaylight.</li>
</ul>

<p>For the setup that we&#39;re going to deploy in upcoming lab sections, we&#39;ve got a partially pre-built set of templates that are going to be used. We&#39;ll be adding to these skeleton files with some additional configuration over the next few lab sections, but we should grab these templates now, making sure we execute this in our new ~/templates directory:</p>

<div><pre><code class="language-none">$ cd ~/templates/
$ cp -rf ~/labs/odl/templates/* ~/templates/</code></pre></div>

<p>There&#39;s one main environment file that has been partially filled out to help us define the lab environment, specifically setting things like the physical networking configuration (to conincide with the physical setup - more on this later), and various other main parameters that we can look into; this is set at <strong>~/templates/config.yaml</strong>.</p>

<p><br /></p>

<h3 id="toc_27">Node to Role Tagging</h3>

<p>OpenStack Nova uses two main metrics to determine whether an Ironic node is suitable for a given role (e.g. controller, compute node, networker), both of which are encapsulated into a flavor. The flavor defines the required size of the machine (e.g. number of CPU&#39;s, memory capacity, and available disk space) and also has a set of properties that a node must also satisfy. For example, a node may have the required physical capacity, but it may not satisfy additional properties, such as having SR/IOV capable network interfaces. In OSP director we use the additional properties to define what we call a <strong>profile</strong>, it&#39;s the profile that we assign to a given node so that Nova doesn&#39;t just have to rely on the minimum hardware specification, we can give it a hint to say these machines are definitely controllers, for example.</p>

<p>Whilst it&#39;s possible to not use profiles at all and have Nova just rely on hardware specifications, or just choose nodes at random, profiles allow us more predictability when it comes to knowing which nodes will become certain roles, and for many organisations it&#39;s exactly what they need. In this lab section we&#39;re going to assign some profiles to our nodes, as our nodes have differing hardware specification and we want to make sure the right nodes are chosen for a given role.</p>

<p>As we just alluded to, Nova relies on a flavor to determine which role to assign to a given node. These flavors are defined by a required hardware specification and a set of properties that align to a profile that each node is assigned. For example, if we look at the available flavors, and the <strong>controller</strong> flavor example:</p>

<div><pre><code class="language-none">$ openstack flavor list
+--------------------------------------+---------------+------+------+-----------+-------+-----------+
| ID                                   | Name          |  RAM | Disk | Ephemeral | VCPUs | Is Public |
+--------------------------------------+---------------+------+------+-----------+-------+-----------+
| 2190e8a9-e7c8-4829-a45c-66d67b214786 | block-storage | 4096 |   40 |         0 |     1 | True      |
| 54a81574-48d7-485b-8918-67e729469e08 | baremetal     | 4096 |   40 |         0 |     1 | True      |
| 75fe95eb-86a2-4c1a-ac8b-e6352b53ee4b | networker     | 4096 |   40 |         0 |     1 | True      |
| 81199b17-dcd4-4ca5-92b0-2287c85af020 | control       | 4096 |   40 |         0 |     1 | True      |
| 8759f6de-7d24-4f99-86dc-5e93a22306dc | ceph-storage  | 4096 |   40 |         0 |     1 | True      |
| 95df47f2-10f0-41a0-b577-02ab48f51400 | compute       | 4096 |   40 |         0 |     1 | True      |
| becf2ea9-af96-405b-aa14-d4c858715181 | swift-storage | 4096 |   40 |         0 |     1 | True      |
+--------------------------------------+---------------+------+------+-----------+-------+-----------+

$ openstack flavor show control
+----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field                      | Value                                                                                                                                                                |
+----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| OS-FLV-DISABLED:disabled   | False                                                                                                                                                                |
| OS-FLV-EXT-DATA:ephemeral  | 0                                                                                                                                                                    |
| access_project_ids         | None                                                                                                                                                                 |
| disk                       | 40                                                                                                                                                                   |
| id                         | 81199b17-dcd4-4ca5-92b0-2287c85af020                                                                                                                                 |
| name                       | control                                                                                                                                                              |
| os-flavor-access:is_public | True                                                                                                                                                                 |
| properties                 | capabilities:boot_option=&#39;local&#39;, capabilities:profile=&#39;control&#39;, resources:CUSTOM_BAREMETAL=&#39;1&#39;, resources:DISK_GB=&#39;0&#39;, resources:MEMORY_MB=&#39;0&#39;, resources:VCPU=&#39;0&#39; |
| ram                        | 4096                                                                                                                                                                 |
| rxtx_factor                | 1.0                                                                                                                                                                  |
| swap                       |                                                                                                                                                                      |
| vcpus                      | 1                                                                                                                                                                    |
+----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+</code></pre></div>

<p>You can see from the above output that the controller flavor (<strong>control</strong>) requires 4GB memory, 1 CPU, and 40GB disk, but relies on the <strong>profile</strong> also called &quot;<strong>control</strong>&quot; (see properties field). Hence when Nova is looking for available baremetal nodes stored by Ironic, it will want to find a node that has been associated with the correct profile. If we look at the currently assigned profiles for our nodes, you&#39;ll see that the profile has already been assigned (as it was done for the pre-deployed environment):</p>

<div><pre><code class="language-none">$ openstack overcloud profiles list -c &quot;Node Name&quot; -c &quot;Current Profile&quot;
+--------------------+-----------------+
| Node Name          | Current Profile |
+--------------------+-----------------+
| summit-controller1 | control         |
| summit-compute1    | compute         |
| summit-compute2    | compute         |
| summit-networker1  | networker       |
+--------------------+-----------------+</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: If you don&#39;t see all four nodes listed here it may be an API problem with the public cloud environment that we&#39;re using. You can reset the node state with the following, before re-running the profiles list command:</p>
</blockquote>

<div><pre><code class="language-none">$ for i in controller1 compute1 compute2 networker1; \
    do ironic node-set-provision-state summit-$i deleted; done</code></pre></div>

<p>As these are already set, we don&#39;t have to make any changes at this point, but to demonstrate how easy it is to set a profile to a node, you can re-run this command:</p>

<div><pre><code class="language-none">$ openstack baremetal node set --property \
    capabilities=&#39;profile:control,boot_option:local&#39; summit-controller1</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: This command should produce no output unless there was an error.</p>
</blockquote>

<h2 id="toc_28">Physical Networking Topology</h2>

<p>Each of our overcloud nodes has been configured with <strong>two</strong> &quot;physical&quot; (only quoted because they&#39;re virtualised baremetal machines) network interfaces:</p>

<ul>
<li><p>The first network interface (<strong>eth0</strong>) is attached to a dedicated, isolated network that is used for bare metal provisioning, known as the <strong>control plane</strong> (ctlplane), i.e. this is the network that is used by Ironic to deploy the nodes via DHCP/PXE, and is used by the nodes to communicate back to the undercloud during initial deployment and any ongoing maintenance tasks. OSP director is assumed to have full control over this network.</p></li>
<li><p>The second network interface (<strong>eth1</strong>) is attached to a network provided by the public cloud platform in which we can both gain routable access to our overcloud nodes when they&#39;re provisioned and also run a number of different VLAN&#39;s on-top of for other OpenStack network traffic types, e.g. internal API communication, or storage access.</p></li>
</ul>

<p>Or visually represented:</p>

<p><img src="images/network-topology.png" style="width: 1000px;"/></p>

<blockquote>
<p><strong>NOTE</strong>: In the above diagram, the green network (where our jump host is connected to, just incase you&#39;re viewing this without colour) represents what would likely be a corporate network in a real-world environment, one that&#39;s routable. But within our virtual lab, this is a flat network with subnet 192.168.122.0/24, and we&#39;ll be able to easily access the nodes and the OpenStack API&#39;s once the overcloud is deployed.</p>
</blockquote>

<p>Prior to this lab (and out of scope for the lab instructions) we defined the virtualised nodes within Ironic, and when they were registered we provided the MAC address of <strong>eth0</strong> for each node as the unique identifier, this ensures that when it does DHCP/PXE boot for deployment, Ironic knows exactly which machine it&#39;s dealing with. During the initial bootstrap of the image onto the booting nodes, this is the only interface we care about. However, once the machine reboots into the image for the next stage of deployment OSP director needs to configure all of the networking interfaces. To do this, OSP director relies on a set of templates known as &#39;<strong>nic-configs</strong>&#39; to set the configuration for each interface, including any additional VLANs, bonds, or bridges. The primary requirement in our lab environment is going to be defining what happens with <strong>eth1</strong> - how the necessary VLANs are defined on-top of this interface, and how OpenStack can be deployed to use them.</p>

<h3 id="toc_29">Out of the Box Options</h3>

<p>TripleO and OSP director ship with a number of <strong>nic-config</strong> templates that can be used as examples to build specific templates for the given environment that they&#39;re being deployed into. Examples include (and can be found in this directory):</p>

<div><pre><code class="language-none">undercloud$ ll /usr/share/openstack-tripleo-heat-templates/network/config/
total 0
drwxr-xr-x. 2 root root 230 Aug 18 08:06 bond-with-vlans
drwxr-xr-x. 2 root root 194 Aug 18 08:06 multiple-nics
drwxr-xr-x. 2 root root 170 Aug 18 08:06 single-nic-linux-bridge-vlans
drwxr-xr-x. 2 root root 205 Aug 18 08:06 single-nic-vlans</code></pre></div>

<p>In each directory there&#39;s a template for each role (at least out of the box roles, <strong>not</strong> taking into consideration composable roles) and slight variations of the roles, e.g. compute with DPDK enabled, or controller with IPv6 support:</p>

<div><pre><code class="language-none">undercloud$ ll /usr/share/openstack-tripleo-heat-templates/network/config/bond-with-vlans/
total 68
-rw-r--r--. 1 root root 5813 Apr 27 21:06 ceph-storage.yaml
-rw-r--r--. 1 root root 6073 Apr 27 21:06 cinder-storage.yaml
-rw-r--r--. 1 root root 6387 Apr 27 21:06 compute-dpdk.yaml
-rw-r--r--. 1 root root 6060 Apr 27 21:06 compute.yaml
-rw-r--r--. 1 root root 6240 Apr 27 21:06 controller-no-external.yaml
-rw-r--r--. 1 root root 6933 Apr 27 21:06 controller-v6.yaml
-rw-r--r--. 1 root root 6676 Apr 27 21:06 controller.yaml
-rw-r--r--. 1 root root 2128 Apr 27 21:06 README.md
-rw-r--r--. 1 root root 6072 Apr 27 21:06 swift-storage.yaml</code></pre></div>

<p>Unfortunately, none of these out of the box templates suit our specific environment, i.e. multiple network interfaces but with a dedicated provisioning interface and a dedicated interface for everything else. These templates either assume that you have just a single network interface, or each network interface is for a different traffic type (common in Cisco UCS environments) or if you want all interfaces bonded together for resilience. Within our virtualised baremetal environment we&#39;ve gone for something slightly different - certainly not recommended for production, but makes it easier to explain how the templates work. As none of these out of the box templates work for us, we&#39;re going to have to use custom nic-config templates to suit our requirements.</p>

<h3 id="toc_30">Pre-prepared nic-config Templates</h3>

<p>In the <strong>~/templates</strong> directory you&#39;ll find a subdirectory called <strong>&#39;nic-configs&#39;</strong>, in which we&#39;ve pre-prepared a set of templates for us to use, ones that match our expectations perfectly. Two network interfaces, first interface being for provisioning (via the OSP director control plane network) and a second interface for running all OpenStack traffic on, including for providing external networking access, i.e. floating IP access, and also OpenStack API access via a routable network from our workstation. Despite these being custom templates they&#39;re heavily built from the &quot;<strong>single-nic-vlans</strong>&quot; example.</p>

<p>Let&#39;s verify that we have these in place:</p>

<div><pre><code class="language-none">$ ll ~/templates/nic-configs/
total 20
-rw-r--r--. 1 stack stack 3951 Dec 1  11:01 compute.yaml
-rw-r--r--. 1 stack stack 4277 Dec 1  11:01 controller.yaml
-rw-r--r--. 1 stack stack 4277 Dec 1  11:01 networker.yaml</code></pre></div>

<p>You&#39;ll notice that this directory is slightly cut down from the examples above. This is primarily because we only have three different roles within our environment - we don&#39;t have dedicated storage nodes, nor are we using IPv6, etc. We have a single controller, a dedicated networker, and two compute nodes, hence why we only have templates that represent the roles that we want to deploy.</p>

<p>Let&#39;s take a look at what these templates actually look like. Just like all other templates, these are <strong>yaml</strong> formatted, and therefore whitespace is incredibly important in describing the hierarchy. Let&#39;s cut the main bit out of the template so we can explain the most important section (but please feel free to look into the entire file with the favourite text editor, just don&#39;t make any modifications before we start our deployment). The first command below will print the entire of the main section, but we&#39;ve split the output to describe each section below.</p>

<p>The top level sector is the &quot;<strong>network_config</strong>&quot; type, where all of the interfaces and sub-interfaces are described programmatically. Underneath this we describe our first interface, <strong>eth0</strong>, and associate it with both an IP address (and netmask) from our <strong>ControlPlane</strong>, the network we use for provisioning, as well as a static route to the metadata service:</p>

<div><pre><code class="language-none">$ grep -A54 network_config ~/templates/nic-configs/controller.yaml
        $network_config:
          network_config:
          - type: interface
            name: eth0
            use_dhcp: false
            addresses:
            - ip_netmask:
                list_join:
                - /
                - - get_param: ControlPlaneIp
                  - get_param: ControlPlaneSubnetCidr
            routes:
            - ip_netmask: 169.254.169.254/32
              next_hop:
                get_param: EC2MetadataIp
         (...)</code></pre></div>

<p>Next, we create an Open vSwitch bridge called <strong>br-ex</strong>, and associate it with an IP address and a default route from the <strong>ExternalNetwork</strong>. We also add <strong>eth1</strong> as the physical interface that backs this bridge. OpenDaylight utilises Open vSwitch on the nodes and therefore the configuration here is identical to a non-OpenDaylight environment, it&#39;s just that the Open vSwitch will be programmed directly by the OpenDaylight controller instead of Neutron&#39;s ML2/OVS implementation. In a later step we&#39;ll tell OSP director to configure OpenDaylight to utilise this bridge.</p>

<p>Any interfaces or sub-interfaces associated with this bridge will be able to egress and ingress via <strong>eth1</strong>.</p>

<div><pre><code class="language-none">            - type: ovs_bridge
            name: br-ex
            use_dhcp: false
            dns_servers:
              get_param: DnsServers
            addresses:
            - ip_netmask:
                get_param: ExternalIpSubnet
            routes:
            - default: true
              next_hop:
                get_param: ExternalInterfaceDefaultRoute
            members:
            - type: interface
              name: eth1
              # force the MAC address of the bridge to this interface
              primary: true
            (...)</code></pre></div>

<p>Then, we add multiple <strong>VLAN</strong> sub-interfaces to this bridge, one for each OpenStack network traffic type, noting that via a parameter it looks up the VLAN ID we want to assign to each traffic type:</p>

<div><pre><code class="language-none">          - type: vlan
              vlan_id:
                get_param: InternalApiNetworkVlanID
              addresses:
              - ip_netmask:
                  get_param: InternalApiIpSubnet
            - type: vlan
              vlan_id:
                get_param: StorageNetworkVlanID
              addresses:
              - ip_netmask:
                  get_param: StorageIpSubnet
            - type: vlan
              vlan_id:
                get_param: StorageMgmtNetworkVlanID
              addresses:
              - ip_netmask:
                  get_param: StorageMgmtIpSubnet
            - type: vlan
              vlan_id:
                get_param: TenantNetworkVlanID
              addresses:
              - ip_netmask:
                  get_param: TenantIpSubnet</code></pre></div>

<p>Remember, whitespace is incredibly important for the network interface hierarchy, and the parameters that should be associated to each interface. When the machine boots up for the first time, this network template is provided to a tool called <strong>os-net-config</strong> which applies this template to the local machine. The above example is for the controller model, in which all VLANs are present, have a look at the compute one to see that there are a limited number of VLANs present.</p>

<blockquote>
<p><strong>NOTE</strong>: OSP director has a lot of granularity when it comes to the physical networks that it utilises for network traffic types, mapped by the <strong>ServiceNetMap</strong>, in which you choose which network is used for each type. For example, you could combine certain traffic types onto one network if the number of VLANs are limited, or you want to minimise the number of network interfaces used. The templates used here satisfy the default network traffic types.</p>
</blockquote>

<p>By default, the <strong>ServiceNetMap</strong> maps the OpenDaylightApi network to the Internal API network, in other words, Open vSwitch on all of the compute nodes and the networker nodes is configured to talk to the OpenDaylight controller on the <strong>Internal</strong> network.</p>

<p>OpenDaylight uses a distributed routing architecture, much like the distributed virtual routing (DVR) capabilities on OVS/ML2+Neutron, therefore each compute node needs to have direct physical access to any network that permits floating IP attachment. Therefore, any bridges that are required to allow this functionality must be configured within the nic-config templates also.</p>

<p>So, we&#39;ve got these nic-config templates, but how do we tell OSP director that we want to actually use them? We can specify this in our environment file, by overriding specific TripleO Heat resource types - unless these are overriden, OSP director assumes that your nodes only have one network interface and carries all traffic over this interface. Inside of our environment file you&#39;ll notice that there&#39;s a section called <strong>&quot;resource_registry&quot;</strong> in which we can do this very thing - override resource types by specifying the location of a Heat template file that describes such a resource:</p>

<div><pre><code class="language-none">undercloud$ grep -A3 resource_registry ~/templates/config.yaml
resource_registry:
  OS::TripleO::Compute::Net::SoftwareConfig: /home/stack/templates/nic-configs/compute.yaml
  OS::TripleO::Controller::Net::SoftwareConfig: /home/stack/templates/nic-configs/controller.yaml
  OS::TripleO::Networker::Net::SoftwareConfig: /home/stack/templates/nic-configs/networker.yaml</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: To reiterate, if you omit to advise OSP director of your nic-configs, the default model is to assume that your systems only have a single network interface and that it should be used for provisioning, control plane functionality, and all OpenStack service traffic. This is not a likely deployment in production, but it can suffice for testing.</p>
</blockquote>

<h2 id="toc_31">Specifying the Network Details</h2>

<p>We&#39;ve got the nic-config templates ready, but they don&#39;t actually specify what IP addresses, VLAN&#39;s, DNS servers, default routes, etc, to use for each of the interface, all it shows is a link to a parameter which we haven&#39;t yet specified. Where do we specify these? Well, just like some of the other parameters, we need to specify these in our environment file. We need to make sure that OSP director knows how to satisfy all of the different network traffic types being requested and therefore need to provide information for each subnet, i.e. which network address range to use, what the default route is (if applicable) and if you&#39;re using VLAN isolation, what VLAN ID it should use for each network. There are also a few additional parameters that are advisable to set during this process.</p>

<p>In our environment we&#39;ve opted to use the following network configuration, where we can specify the subnet size, the VLAN ID, and a range of IP&#39;s to use in that pool. In addition, we&#39;ve specified the network size and default route of the control plane network, as well as the DNS servers to use and the default name for the external network bridge:</p>

<div><pre><code class="language-none">$ grep -A27 &quot;Internal API&quot; ~/templates/config.yaml
  # Internal API used for private OpenStack Traffic
  InternalApiNetCidr: 172.17.1.0/24
  InternalApiAllocationPools: [{&#39;start&#39;: &#39;172.17.1.10&#39;, &#39;end&#39;: &#39;172.17.1.200&#39;}]
  InternalApiNetworkVlanID: 101
  InternalApiNetworkVip: 172.17.1.150

  # Tenant Network Traffic - will be used for VXLAN over VLAN
  TenantNetCidr: 172.17.2.0/24
  TenantAllocationPools: [{&#39;start&#39;: &#39;172.17.2.10&#39;, &#39;end&#39;: &#39;172.17.2.200&#39;}]
  TenantNetworkVlanID: 201

  # Public Storage Access - e.g. Nova/Glance &lt;--&gt; Ceph
  StorageNetCidr: 172.17.3.0/24
  StorageAllocationPools: [{&#39;start&#39;: &#39;172.17.3.10&#39;, &#39;end&#39;: &#39;172.17.3.200&#39;}]
  StorageNetworkVlanID: 301
  StorageNetworkVip: 172.17.3.150

  # Private Storage Access - i.e. Ceph background cluster/replication
  StorageMgmtNetCidr: 172.17.4.0/24
  StorageMgmtAllocationPools: [{&#39;start&#39;: &#39;172.17.4.10&#39;, &#39;end&#39;: &#39;172.17.4.200&#39;}]
  StorageMgmtNetworkVlanID: 401
  StorageMgmtNetworkVip: 172.17.4.150

  # External Networking Access - Public API Access
  ExternalNetCidr: 192.168.122.0/24
  ExternalAllocationPools: [{&#39;start&#39;: &#39;192.168.122.102&#39;, &#39;end&#39;: &#39;192.168.122.129&#39;}]
  ExternalInterfaceDefaultRoute: 192.168.122.1
  ExternalNetworkVip: 192.168.122.100</code></pre></div>

<p>As an example, you can see that the external network has been configured such that the subnet specification is <strong>192.168.122.0/24</strong>, with IP&#39;s available between 192.168.122.100 and 192.168.122.129, and that the <strong>default route</strong> is 192.168.122.1. Note also that this doesn&#39;t have a VLAN associated with it as it&#39;s a flat network on the public cloud platform.</p>

<h2 id="toc_32">OpenDaylight Specifics</h2>

<p>So far, pretty much all of the TripleO configuration that we&#39;ve used is identical to that of a non-OpenDaylight environment, and would suffice for configuring a standard Neutron with the OVS/ML2 networking implementation. But the whole purpose of this lab is to get OpenDaylight integrated with our overcloud, so surely we have to configure that too, right?</p>

<p>Well, actually the current configuration options are relatively limited; there are many configuration options that make no sense for us to deviate from the defaults, such as the default ports and communication protocols, and some options we cannot enable, such as DPDK, simply because we don&#39;t have easy access to such hardware. A list of the current configuration options exposed by TripleO are shown below:</p>

<table>
<thead>
<tr>
<th>Parameter</th>
<th>Details</th>
</tr>
</thead>

<tbody>
<tr>
<td><strong>OpenDaylightPort</strong></td>
<td>This dictates the port that OpenDaylight uses on the Northbound traffic, i.e. the port that Open vSwitch communicates with OpenDaylight on. This defaults to <strong>8081</strong>.</td>
</tr>
<tr>
<td><strong>OpenDaylightUsername</strong></td>
<td>This sets the username that OpenDaylight uses for management of the SDN controller. Defaults to &quot;<strong>admin</strong>&quot; if left unset.</td>
</tr>
<tr>
<td><strong>OpenDaylightPassword</strong></td>
<td>This sets the password that goes in combination with the username set in <strong>OpenDaylightUsername</strong>, but defaults to &quot;<strong>admin</strong>&quot; if left unset.</td>
</tr>
<tr>
<td><strong>OpenDaylightEnableDHCP</strong></td>
<td>This enables DHCP functionality provided by OpenDaylight itself. Unfortunately this is not currently supported, so the parameter is more of a placeholder at this time. OpenDaylight still relies on Neutron&#39;s default DHCP agent for implementation. Hence it defaults to <strong>false</strong>, but is ignored.</td>
</tr>
<tr>
<td><strong>OpenDaylightFeatures</strong></td>
<td>This allows the administrator to specify the features that OpenDaylight enables, recalling that OpenDaylight has a very modular and flexible architecture. This defaults to <strong>[odl-netvirt-openstack, odl-netvirt-ui, odl-jolokia]</strong>, the minimum set of features to meet parity with OVS/ML2 in terms of OpenStack networking integration. Currently, Red Hat only package the list that&#39;s provided above, so it&#39;s not a good idea to modify this list at this time.</td>
</tr>
<tr>
<td><strong>OpenDaylightConnectionProtocol</strong></td>
<td>This sets the connection protocol that OpenDaylight can be contacted on for API access, i.e. Layer7. This defaults to &#39;<strong>http</strong>&#39;.</td>
</tr>
<tr>
<td><strong>OpenDaylightManageRepositories</strong></td>
<td>This either enables or disables the upstream OpenDaylight repositories for plugins and modules, this defaults to <strong>false</strong>.</td>
</tr>
<tr>
<td><strong>OpenDaylightSNATMechanism</strong></td>
<td>This sets the mechanism in which OpenDaylight provides SNAT support for Neutron networks. This defaults to using <strong>&#39;conntrack&#39;</strong>, which distributes SNAT to compute nodes within the cluster using OVS-based firewalling, but this option also supports <strong>&#39;controller&#39;</strong> mode which punts this functionality to the controllers. Useful for situations in which non-kernel based OVS needs to be used, e.g. DPDK environments.</td>
</tr>
<tr>
<td><strong>OpenDaylightCheckURL</strong></td>
<td>This sets the URL that OpenDaylight exposes for clients to check that OpenDaylight has come up properly and is ready to function as the controller. This defaults to &#39;<strong>restconf/operational/network-topology:network-topology/topology/netvirt:1</strong>&#39;.</td>
</tr>
<tr>
<td><strong>OpenDaylightProviderMappings</strong></td>
<td>This tells OpenDaylight how to map Neutron logical network labels, e.g. <strong>physnet1</strong> to a configured bridge on the hosts. This parameter is very important to ensure proper networking functionality. By default, OSP director creates a logical Neutron network label called <strong>datacentre</strong>, and this needs to be mapped to an OVS bridge set in the nic-configs.</td>
</tr>
<tr>
<td><strong>OvsEnableDpdk</strong></td>
<td>This parameter, whilst not strictly an OpenDaylight parameter, can be used to enable the DPDK functionality with Open vSwitch, for collaboration with OpenDaylight. This defaults to <strong>false</strong> if unset.</td>
</tr>
<tr>
<td><strong>HostAllowedNetworkTypes</strong></td>
<td>Again, not strictly an OpenDaylight parameter, but is important as it defines the network types that are permitted to be created by Neutron. This defaults to &#39;<strong>[&#39;local&#39;, &#39;vlan&#39;, &#39;vxlan&#39;, &#39;gre&#39;]</strong>&#39; if left unset.</td>
</tr>
</tbody>
</table>

<p>Based on these options, let&#39;s make some slighy changes to the default configuration that&#39;s provided by the TripleO templates. By default, OSP director creates a logical Neutron network label called <strong>datacentre</strong>, so we will need to map this logical name to an Open vSwitch bridge that OpenDaylight can control:</p>

<div><pre><code class="language-none">$ echo &quot;  OpenDaylightProviderMappings: &#39;datacentre:br-ex&#39;&quot; &gt;&gt; ~/templates/config.yaml</code></pre></div>

<p>Next, let&#39;s override the default username/password combination for our OpenDaylight controller:</p>

<div><pre><code class="language-none">$ echo &quot;  OpenDaylightUsername: admin&quot; &gt;&gt; ~/templates/config.yaml
$ echo &quot;  OpenDaylightPassword: redhat&quot; &gt;&gt; ~/templates/config.yaml</code></pre></div>

<p>That&#39;s it! Our nodes are ready, our templates have been explored and customised to suit, and we understand the options that we could have chosen. Let&#39;s move onto the deployment itself.</p>

<h2 id="toc_33">OpenStack Deployment with OpenDaylight</h2>

<p>In this section we&#39;re going to be deploying our Red Hat OpenStack Platform overcloud with an integrated OpenDaylight SDN controller. In the previous step we explored the main physical characteristics of the environment that we&#39;re working with and then configured the vast majority of the templates that are required for deployment in such an environment. Before we go ahead with our deployment we need to wrap up a couple of last minute customisations.</p>

<h3 id="toc_34">OpenStack Service Containerisation</h3>

<p>One of the biggest changes in OSP12 is the containerisation of OpenStack services, i.e. OpenStack services residing and operating within docker-based containers. This is a huge feature of OSP12, and a huge architectural change. Red Hat are embracing it for many reasons...</p>

<p>Firstly, we get dependency isolation for each service - we can embed everything that we need for a given service into a single container image, without having to worry that we’ll break other service functionality, including required library versions, or anything that may be required. It makes updates and upgrades a lot easier to manage - a container can simply be replaced with a newer copy, containing the newer or patched code, and if that operation fails, it’s very simple to roll back to the previous version, without a panic about how to restore the environment. We get a higher degree of deployment flexibility, building on-top of the composable roles functionality we can distribute and rebalance services at will. Scalability is also much easier, we can throw more containers into the mix to accommodate demand when required, and scale back when not, efficiently using hardware. Because we’re using immutable infrastructure, i.e. when it’s running it doesn’t change you need to rip and replace to make a change, it means that the code and configuration is well understood, and it means that the complexity around configuration file management and day to day operations is minimised. Finally, we can also leverage a lot of the new container runtime management technology for better resource utilisation, and control over allocation of system resources. To sum things up, containers are bringing a huge benefit to our customers and Red Hat&#39;s ability to support and maintain OpenStack.</p>

<h3 id="toc_35">What&#39;s required for Containerised Services?</h3>

<p>By default in OSP12+, each OpenStack service has it&#39;s own self-contained Docker image, containing all required dependencies. Docker images are stored in repositories known as registries and need to be <strong>pulled</strong> before using them. All of the required images have been pulled for you, and reside in a local registry on the undercloud machine. Whilst it&#39;s possible to use a remote Docker registry during deployment, it&#39;s not very efficient given that the images can be quite large, and also the nodes may not have a network route to the registry. By using the undercloud server as a registry we gain performance and a guaranteed network route.</p>

<p>The typical flow of an administrator wanting to deploy a containerised overcloud is as follows-</p>

<ol>
<li>Generate a list of required images that will need to be used based on the chosen TripleO configuration, e.g. if using OpenDaylight, make sure you include this in your required images list.<br /><br /></li>
<li>Take this list and upload the images to the <strong>local</strong> docker registry (on the undercloud) from the <strong>remote</strong> registry (e.g. the Red Hat Content Delivery Network).<br /><br /></li>
<li>Generate an environment file used by TripleO; typically stored as &quot;<strong>docker_registry.yaml</strong>&quot;, that tells OSP director exactly which docker images to use for each TripleO service and where to find them, i.e. the image location, the name, and the tag on the <strong>local</strong> registry. It&#39;s used to tell the booting overcloud nodes which images to pull and where to pull them from.<br /><br /></li>
<li>Deploy the overcloud, specifying the generated environment file (i.e. docker_registry.yaml).</li>
</ol>

<p>Steps (1), (2), and (3) can be taken using TripleO command line tooling, but for convenience we&#39;ve already taken these steps, and you&#39;ll find the correct docker_registry.yaml file in your <strong>~/templates/</strong> directory. As an example, let&#39;s ensure that it has the OpenDaylight container configured correctly:</p>

<div><pre><code class="language-none">$ grep -i OpenDaylight ~/templates/docker_registry.yaml | grep -v &quot;^#&quot;
  DockerNeutronApiImage: 172.16.0.1:8787/rhosp12/openstack-neutron-server-opendaylight:12.0-20180319.1
  DockerNeutronConfigImage: 172.16.0.1:8787/rhosp12/openstack-neutron-server-opendaylight:12.0-20180319.1
  DockerOpendaylightApiImage: 172.16.0.1:8787/rhosp12/openstack-opendaylight:12.0-20180319.1
  DockerOpendaylightConfigImage: 172.16.0.1:8787/rhosp12/openstack-opendaylight:12.0-20180319.1</code></pre></div>

<p>Now we have four images listed, why four? Well, each service upon TripleO instantiation needs to do two things - firstly, it needs to configure itself, e.g. setting up configuration files via puppet and running bootstrap commands (e.g. Galera bootstrap, RabbitMQ configuration, etc), and secondly there has to be an image that is used to run the OpenStack service itself - one that contains the binaries. In the vast majority of cases, this image is the same across both the first phase configuration step, and the second phase actually running of the binaries/services.</p>

<h3 id="toc_36">Let&#39;s do the deployment...</h3>

<p>That&#39;s it, we&#39;ve identified all of the docker images that are required, we&#39;ve uploaded the images to the local docker registry, and we&#39;ve generated a TripleO environment file to point OSP director at those images for utilisation by the overcloud nodes. We can now start the deployment. Ensure that you&#39;re in the home directory of the stack user (to keep the ~/templates directory clean) and execute the deploy command:</p>

<div><pre><code class="language-none">$ cd ~
$ openstack overcloud deploy --templates \
    -r ~/templates/lab_roles.yaml \
    -e ~/templates/config.yaml \
    -e ~/templates/docker_registry.yaml \
    -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \
    -e /usr/share/openstack-tripleo-heat-templates/environments/services-docker/neutron-opendaylight.yaml
(...)</code></pre></div>

<p>The deployment takes approximately 60 minutes (on the public cloud platform that we&#39;re using), so depending on how long is left in the lab session you may not get to see this finish, but by getting to this stage you should have a good understanding of what the integration looks like, and how to deploy it. For reference (and for reading whilst the deployment is taking place), the command line options for the deploy command are explained below in more detail:</p>

<table>
<thead>
<tr>
<th>Parameter</th>
<th>Details</th>
</tr>
</thead>

<tbody>
<tr>
<td>--templates</td>
<td>This tells OSP director that you want to use the TripleO Heat Templates for deployment, from the default location <strong>/usr/share/openstack-tripleo-heat/templates</strong> (unless overridden with a different directory path)</td>
</tr>
<tr>
<td>-r ~/templates/custom_roles.yaml</td>
<td>The &#39;<strong>-r</strong>&#39; flag tells the command line tooling that you want to define the roles manually, rather than utilising the out of the box roles. This points to our <strong>~/templates/custom_roles.yaml</strong> file which we are using to create the dedicated networker node.</td>
</tr>
<tr>
<td>-e ~/templates/config.yaml</td>
<td>The &#39;<strong>-e</strong>&#39; flag tells OSP director that you&#39;re specifying an environment file that will modify the out of the box configuration, it can be used to specify (and override) standard parameters and TripleO resources with custom values and custom templates (e.g. nic-configs). In this section we&#39;re providing our custom environment file containing links to our custom nic-configs and all associated parameters that we&#39;ve described over the past few lab sections.</td>
</tr>
<tr>
<td>-e ~/templates/docker_registry.yaml</td>
<td>The environment file that specifies the list of docker images that will be used within the overcloud based on the desired TripleO configuration, including the location of the images, i.e. the local docker registry.</td>
</tr>
<tr>
<td>-e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml</td>
<td>An environment file that we&#39;ve not seen before, and is an out of the box example environment file - this allows us to use the default network isolation mechanism, i.e. a dedicated network (and associated ports) for each network traffic type.</td>
</tr>
<tr>
<td>-e /usr/share/openstack-tripleo-heat-templates/environments/services-docker/neutron-opendaylight.yaml</td>
<td>This is another environment file that specifies that we want to use OpenDaylight as part of this deployment. This environment file explicitly disables Neutron&#39;s out of the box Open vSwitch configuration, along with non-used supporting agents such as Neutron&#39;s L3 agent. It also sets specific parameters so that Neutron can use OpenDaylight.</td>
</tr>
</tbody>
</table>

<p><br />
Once the deployment has succeeded, you should receive the following output:</p>

<div><pre><code class="language-none">(...)
2017-11-27 12:36:56Z [overcloud.AllNodesDeploySteps]: CREATE_COMPLETE  Stack CREATE completed successfully
2017-11-27 12:36:56Z [overcloud.AllNodesDeploySteps]: CREATE_COMPLETE  state changed
2017-11-27 12:36:56Z [overcloud]: CREATE_COMPLETE  Stack CREATE completed successfully

 Stack overcloud CREATE_COMPLETE

Host 192.168.122.100 not found in /home/stack/.ssh/known_hosts
Overcloud Endpoint: http://192.168.122.100:5000/v2.0
Overcloud Deployed</code></pre></div>

<blockquote>
<p><strong>NOTE</strong>: If you do not receive a &quot;<strong>CREATE_COMPLETE</strong>&quot; status output then unfortunately your deployment has failed. This may be because of a syntax error in your templates. Run the following command to output the failures that were caught.</p>

<div><pre><code class="language-none">  undercloud$ openstack stack failures list overcloud --long</code></pre></div>
</blockquote>

<p><br />
If you&#39;ve got this far, we&#39;re assuming that you&#39;ve successfully built your OpenDaylight integrated Red Hat OpenStack Platform overcloud. We have provided a test script that will quickly build some resources to test that everything worked correctly; you can execute it as follows:</p>

<div><pre><code class="language-none">$ sh ~/labs/odl/test-overcloud.sh
(...)

+--------------------------------------+---------+--------+---------------------------------------+-------+---------+
| ID                                   | Name    | Status | Networks                              | Image | Flavor  |
+--------------------------------------+---------+--------+---------------------------------------+-------+---------+
| 638b3a19-f768-4d31-87e3-cca6e99f0197 | test_vm | ACTIVE | internal=172.16.1.11, 192.168.122.203 | rhel7 | m1.labs |
+--------------------------------------+---------+--------+---------------------------------------+-------+---------+</code></pre></div>

<p>The final thing that it should do if things were successful is print out a list of instances, as demonstrated above. A simple ping check should verify if your OpenDaylight deployment was successful, noting that your IP address may be different as it&#39;s not consistently assigned:</p>

<div><pre><code class="language-none">$ ping -c4 192.168.122.203
PING 192.168.122.203 (192.168.122.203) 56(84) bytes of data.
64 bytes from 192.168.122.203: icmp_seq=1 ttl=64 time=0.521 ms
64 bytes from 192.168.122.203: icmp_seq=2 ttl=64 time=0.834 ms
(...)</code></pre></div>

<h2 id="toc_37">Wrapping Up</h2>

<p>That&#39;s it - we&#39;re done! We need to clean up the Heat stack, and we&#39;ll take care of the rest before some of the other labs can proceed later in the day...</p>

<div><pre><code class="language-none">$ openstack stack delete multiple
Are you sure you want to delete this stack(s) [y/N]? yes</code></pre></div>

<p>Thank you very much for attending this lab, I hope that it gave you a bit of insight into how to use OpenStack and OpenDaylight, how its components fit together and how it all works with practical examples. If you have any feedback please share it with us, and if there&#39;s anything we can do to assist you in your OpenStack journey, please don&#39;t hesitate to ask!</p>




</body>

</html>
